{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vapGWIBgFt0q"
   },
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "## TP1 Automne 2020 - Duplicate Bug Report Detection\n",
    "\n",
    "\n",
    "###     Ali Arbaoui \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgkUX6Ki0W9s"
   },
   "source": [
    "## 1 - Résumé / Overview\n",
    "\n",
    "À cause de la complexité des systèmes informatiques, les bogues logiciels sont courants. Les entreprises, et en particulier les plus grosses, utilisent un *Bug Tracking System* (BTS), aussi appelé *Issue Tracking System*, pour organiser et suivre les rapports de bogues. En plus des développeurs et des testeurs, de nombreux projets (notamment ceux libres de droits) permettent aux utilisateurs de soumettre un rapport de bogues dans leur BTS. Pour ce faire, les utilisateurs doivent remplir un formulaire avec plusieurs champs. La majorité de ces champs contient des données catégoriques et accepte uniquement des valeurs prédéfinies (composant, version du produit et du système, etc.). Deux autres champs importants sont le résumé ou *summary* en anglais, et la description. Les utilisateurs sont libres d’écrire ce qu’ils veulent dans ces deux champs avec pour seule contrainte le nombre de caractères. La soumission de ces champs crée une page que l’on appelle rapport de bogue et qui contient toute l’information à propos du bogue.\n",
    "\n",
    "Par manque de communication et de synchronisation, les utilisateurs ne savent pas toujours qu’un bogue a déjà été soumis et peuvent donc le soumettre à nouveau. Identifier les rapports qui correspondent au même bogue (duplicata) est une tache importante des BTSs et est le sujet de ce TP. Notre objectif est de développer un système qui comparera les nouveaux rapports de bogues avec ceux déjà soumis en fonction de leur similarité textuelle. La liste triée des rapports les plus similaires sera utilisée par un opérateur humain pour identifier manuellement si un rapport est un duplicata.\n",
    "\n",
    "---\n",
    "\n",
    "Due to the complexity of software systems, software bugs are prevalent. Companies, especially the larger ones, usually use a Bug Tracking System (BTS), also called Issue Tracking System, to manage and track records of bugs. Besides developers and testers, many projects, mainly open source projects, allow users to report new bugs in their BTS.\n",
    "To do that, users have to fill out a form with multiple fields. An important subset of\n",
    "these fields provides categorical data and only accepts values that range from a fixed list of\n",
    "options (e.g. component, version and product of the system). Two other important fields\n",
    "are the summary and the description. The users are free to write anything in both fields\n",
    "with the only constraint that the summary has a maximum number of characters. The\n",
    "submission of a form creates a page, called bug report or issue report which contains all\n",
    "the information about a bug.\n",
    "\n",
    "Due to the lack of communication and synchronization, users may not be aware that a specific bug was already submitted and may report it again. Identifying duplicate bug reports is an important task in the BTSs and is the subject of this TP. Our objective is to develop a system that will compare a new bug report with the already submitted ones and rank them based on textual similarity. This ranked list will be used by a triager to manually identify whether a report is a duplicate or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFsBsrFzFt0r"
   },
   "source": [
    "# 2 Installation / Setup\n",
    "\n",
    "Pour ce TP, vous aurez besoin des librairies `numpy`, `sklearn` et `scipy` (que vous avez sans doute déjà), ainsi que la librairie `nltk`, qui est une libraire utilisée pour faire du traitement du language (Natural Language Processing, NLP). Installez les libraires en question et exécutez le code ci-dessous :\n",
    "\n",
    "---\n",
    "\n",
    "For this assignment, you need the `numpy`, `sklearn` and `scipy` libraries (which you may already have), as well as the `nltk` library, which is used for Natural Language Processing (NLP). Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8r8BtgdFt0u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\aliar\\appdata\\roaming\\python\\python39\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from scipy) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jupyterthemes in c:\\users\\aliar\\appdata\\roaming\\python\\python39\\site-packages (0.20.0)\n",
      "Requirement already satisfied: lesscpy>=0.11.2 in c:\\users\\aliar\\appdata\\roaming\\python\\python39\\site-packages (from jupyterthemes) (0.15.0)\n",
      "Requirement already satisfied: ipython>=5.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterthemes) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterthemes) (4.9.2)\n",
      "Requirement already satisfied: notebook>=5.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterthemes) (6.4.8)\n",
      "Requirement already satisfied: matplotlib>=1.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterthemes) (3.5.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (61.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.18.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (2.11.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.2)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.4.4)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.20)\n",
      "Requirement already satisfied: stack-data in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.4.1->jupyterthemes) (0.8.3)\n",
      "Requirement already satisfied: ply in c:\\users\\aliar\\appdata\\roaming\\python\\python39\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.21.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.3.2)\n",
      "Requirement already satisfied: argon2-cffi in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (21.3.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (1.5.5)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (1.8.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.3)\n",
      "Requirement already satisfied: nbconvert in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.4.4)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.13.1)\n",
      "Requirement already satisfied: nbformat in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (5.3.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (0.13.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.1)\n",
      "Requirement already satisfied: ipykernel in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.9.1)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.12)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterthemes) (302)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.2.5)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=5.6.0->jupyterthemes) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\programdata\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=5.6.0->jupyterthemes) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=5.6.0->jupyterthemes) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=5.6.0->jupyterthemes) (2.21)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel->notebook>=5.6.0->jupyterthemes) (1.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (2.0.1)\n",
      "Requirement already satisfied: testpath in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (4.11.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.13)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.5.0)\n",
      "Requirement already satisfied: bleach in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (4.1.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.4)\n",
      "Requirement already satisfied: fastjsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (2.15.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->notebook>=5.6.0->jupyterthemes) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->notebook>=5.6.0->jupyterthemes) (0.18.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert->notebook>=5.6.0->jupyterthemes) (2.3.1)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\n",
      "Requirement already satisfied: executing in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.4.1->jupyterthemes) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.4.1->jupyterthemes) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.4.1->jupyterthemes) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jupyterthemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGq_fsnCFt02"
   },
   "source": [
    "## 3 - Jeu de données / Data\n",
    "\n",
    "Téléchargez l'archive à l'adresse suivante: https://drive.google.com/file/d/14BrFaOiVDIcpjwpm5iAkAy0v_moZyGiP/view?usp=sharing\n",
    "\n",
    "In this zip file, there are: \n",
    "\n",
    "L'archive contient:\n",
    "1. test.json: Ce fichier contient les duplicate bug reports that will be used to evaluate our system.\n",
    "2. threads: Ce dossier contient le code HTML des bug report. Chaque fichier HTML est nommé selon le motif **bug_report_id.html**.\n",
    "\n",
    "\n",
    "L'image ci-dessous illustre un exemple de bug report:\n",
    "\n",
    "![https://ibb.co/tqyRM4L](bug_report.png)\n",
    "\n",
    "- A : identifiant du bug report\n",
    "- B : date de création\n",
    "- C : résumé\n",
    "- D : composant\n",
    "- E : produit\n",
    "- F : l'identifiant du rapport dont le bug report est dupliqué\n",
    "- G : description\n",
    "\n",
    "\n",
    "Le script suivant charge le jeu de données de test et définit certaines variables globales:\n",
    "\n",
    "---\n",
    "\n",
    "Please download the zip file at the following adress: https://drive.google.com/file/d/14BrFaOiVDIcpjwpm5iAkAy0v_moZyGiP/view?usp=sharing\n",
    "\n",
    "This archive contains: \n",
    "\n",
    "1. test.json: This file contains duplicate bug reports that will be used to evaluate our system.\n",
    "2. bug_reports: It is a folder that contains the bug report html source. Each html file name follows the pattern **bug_report_id.html**.\n",
    "\n",
    "\n",
    "Figure below depicts an bug report page example:\n",
    "\n",
    "![https://ibb.co/tqyRM4L](bug_report.png)\n",
    "\n",
    "\n",
    "- A : bug report id\n",
    "- B : creation date\n",
    "- C : summary\n",
    "- D : component\n",
    "- E : product\n",
    "- F : the report id which the bug report is duplicate\n",
    "- G : description\n",
    "\n",
    " The following script loads the test dataset and define some global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdRjO-NbFt04"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the folder path that contain the data\n",
    "# FOLDER_PATH = \"Define folder path that contain threads folder and test.json\"\n",
    "FOLDER_PATH = \"dataset/\"\n",
    "PAGE_FOLDER = os.path.join(FOLDER_PATH, 'bug_reports')\n",
    "\n",
    "\n",
    "# Load the evaluation dataset\n",
    "import json\n",
    "os.path.exists(\"test.json\")\n",
    "\n",
    "test = json.load(open(os.path.join(FOLDER_PATH, \"test.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1WXzfKOFt08"
   },
   "source": [
    "# 4 - Web scraping\n",
    "\n",
    "\"Le *web scraping* (parfois appelé harvesting) est une technique d'extraction du contenu de sites Web, via un script ou un programme, dans le but de le transformer pour permettre son utilisation dans un autre contexte, par exemple le référencement.\" [Wikipedia](https://fr.wikipedia.org/wiki/Web_scraping)\n",
    "\n",
    "---\n",
    "\n",
    "*Web scraping* also called harvesting consists in extracting relevant data from web pages and prepare it for computational analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyWZeHkdFt08"
   },
   "source": [
    "## 4.1 - Question 1 (2 points)\n",
    "\n",
    "Implémentez *extract_data_from_page*. Cette fonction extrait l'information suivante du code HTML : identifiant du rapport de bogue, date de création, titre, produit, composant, identifiant du rapport de bogue bug dont il est un duplicata, résumé et description. \n",
    "\n",
    "\n",
    "La fonction *extract_data_from_page* retourne un dictionnaire avec la structure suivante :\n",
    "```\n",
    " {\"report_id\": int, \n",
    "  \"dup_id\": int or None (the report id which it is duplicate), \n",
    "  \"component\": string, \n",
    "  \"product\": string, \n",
    "  \"summary\": str, \n",
    "  \"description\": string, \n",
    "  \"creation_date\": int} \n",
    "```\n",
    "\n",
    "Par exemple, quand la fonction *extract_data_from_page* reçoit le rapport \"bug_report/7526.html\", elle retourne :\n",
    "\n",
    "```\n",
    "{\n",
    "\"report_id\": 7526,\n",
    "\"dup_id\": 7799,\n",
    "\"product\": \"core graveyard\",\n",
    "\"component\":  tracking,\n",
    "\"summary\": \"Apprunner crashes on exit\",\n",
    "\"description\": \"Apprunner crashes on exit, without fail. The browser window closes, but the\n",
    "console window hangs around. I end up having to kill winoldap with the \\\\\"Close\n",
    "Program\\\\\" dialog (Ctrl-Alt-Del).\",\n",
    "\"creation_date\": 928396140\n",
    "}\n",
    "```\n",
    "\n",
    "**La date de création doit être représentée comme un timestamp (entier). Si un bug n'est pas un duplicata, alors dup_id doit être None.**\n",
    "\n",
    "*Indice: lxml parse est plus rapide que html.parser*\n",
    "\n",
    "---\n",
    "\n",
    "Implement *extract_data_from_page*. This function extracts the following information from the html: bug report id, creation date, title, product, component, the report id which it is duplicate, summary and description.\n",
    "\n",
    "The *extract_data_from_page* function returns a dictionary with the following structure:\n",
    "```\n",
    " {\"report_id\": int, \n",
    "  \"dup_id\": int or None (the report id which it is duplicate), \n",
    "  \"component\": string, \n",
    "  \"product\": string, \n",
    "  \"summary\": str, \n",
    "  \"description\": string, \n",
    "  \"creation_date\": int} \n",
    "```\n",
    "\n",
    "For instance, when extract_data_from_page receives \"bug_report/7526.html\", it returns:\n",
    "\n",
    "```\n",
    "{\n",
    "\"report_id\": 7526,\n",
    "\"dup_id\": 7799,\n",
    "\"product\": \"core graveyard\",\n",
    "\"component\":  tracking,\n",
    "\"summary\": \"Apprunner crashes on exit\",\n",
    "\"description\": \"Apprunner crashes on exit, without fail. The browser window closes, but the\n",
    "console window hangs around. I end up having to kill winoldap with the \\\\\"Close\n",
    "Program\\\\\" dialog (Ctrl-Alt-Del).\",\n",
    "\"creation_date\": 928396140\n",
    "}\n",
    "```\n",
    "\n",
    "**Creation date have to be represented as timestamp. If bug report is not duplicate, dup_id have to be None.**\n",
    "\n",
    "*HINT: lxml parse is faster than html.parser*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKv3_jACFt0_"
   },
   "outputs": [],
   "source": [
    "def extract_data_from_page(pagepath):\n",
    "    \"\"\"\n",
    "    Scrap bug report content from bug report html.\n",
    "    \n",
    "    :param pagepath: the path of html file.\n",
    "    :return: \n",
    "        {\n",
    "        \"report_id\": int,\n",
    "        \"dup_id\": int or None (the report id which it is duplicate), \n",
    "        \"component\": string, \n",
    "        \"product\": string, \n",
    "        \"summary\": str, \n",
    "        \"description\": string, \n",
    "        \"creation_date\": int\n",
    "        }\n",
    "    \"\"\"\n",
    "    with open(pagepath,'r', encoding=\"utf8\") as html_file:\n",
    "        content = html_file.read()\n",
    "        \n",
    "    bug_scrape = BeautifulSoup(content,'lxml')\n",
    "    \n",
    "    # Scraping parameters\n",
    "    report_id = int(bug_scrape.find('span', id = 'field-value-bug_id').text.split()[1])\n",
    "    creation_date = int(bug_scrape.find('span', class_ = 'rel-time').get('data-time'))\n",
    "    product = bug_scrape.find('span', id ='product-name').text.split()[0] # component and product texts contain whitespaces and new lines that's why we use split\n",
    "    component = bug_scrape.find('span', id = 'component-name').text.split()[0]\n",
    "    summary = bug_scrape.find('h1', id = 'field-value-short_desc').text\n",
    "    description = bug_scrape.find('pre', id = 'ct-0').text\n",
    "    \n",
    "    # dup_id\n",
    "    if 'duplicate' in bug_scrape.find('span', id = 'field-value-status-view').text.lower():\n",
    "        dup_id = int(bug_scrape.find('span', id = 'field-value-status-view').a.text.split()[1])\n",
    "    else:\n",
    "        dup_id = None\n",
    "    \n",
    "    return {\n",
    "        \"report_id\": report_id,\n",
    "        \"dup_id\": dup_id, \n",
    "        \"component\": component, \n",
    "        \"product\": product, \n",
    "        \"summary\": summary, \n",
    "        \"description\": description, \n",
    "        \"creation_date\": creation_date\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are gonna test on some buggs to see if our fucntions works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'report_id': 63, 'dup_id': None, 'component': 'Aurora/RDF', 'product': 'MozillaClassic', 'summary': 'NavCenter closing up garbage-resource-string', 'description': 'Created by Jukka Santala (donwulff@iki.fi) on Tuesday, April 7, 1998 5:13:38 AM PDT\\nAdditional Details :\\nSometimes (don\\nUpdated by Mike Pinkerton (pinkerton@netscape.com) on Tuesday, April 7, 1998 8:40:37 AM PDT\\nAdditional Details :\\nThis is probably cross platform. I have seen that on my mac builds as well.\\nChanging platform/OS to \"ALL\".\\nUpdated by Jukka Santala (donwulff@iki.fi) on Wednesday, April 8, 1998 6:00:55 PM PDT\\nAdditional Details :\\nFor some reason, adding the previous comment ate most of the original problem\\ndescription; consider that a bug report for the Bugzilla code. In short though,\\n/modules/rdf/src/nlcstore.c has at position 210 rows into itself code, where\\nduring Navigator shutdown resourceID(u) (which expands to ->url macro) sometimes\\nreturns \"garbage\". At first I thought it was just a NULL pointer, but after\\nadding XP_ASSERT()\\'\\'s for this case, Navigator crashed again, this time with\\nresourceID(u)==0xdddddddd', 'creation_date': 891976418}\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(extract_data_from_page('dataset/bug_reports/63.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uU7f5OIzFt1E"
   },
   "source": [
    "## 4.3 - Extraire le texte du code HTML / Extract text from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9w8BxGjgFt1E",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9998/9998 [10:10<00:00, 16.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import json\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Index each thread by its id\n",
    "index_path = os.path.join(PAGE_FOLDER, 'bug_reports.json').replace('\\\\','/')\n",
    "\n",
    "\n",
    "if os.path.isfile(index_path):\n",
    "    # Load threads that webpage content were already extracted.\n",
    "    report_index = json.load(open(index_path))\n",
    "    \n",
    "else:\n",
    "    # Extract webpage content\n",
    "\n",
    "    # This can be slow (around 10 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
    "    files = [os.path.join(PAGE_FOLDER, filename).replace('\\\\','/') for filename in os.listdir(PAGE_FOLDER)]\n",
    "    reports = [extract_data_from_page(f) for f in tqdm(files)]\n",
    "    report_index = dict(((report['report_id'], report) for report in reports ))\n",
    "\n",
    "    # Save preprocessed bug reports\n",
    "    json.dump(report_index, open(index_path,'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pd5Sx79Ft1G"
   },
   "source": [
    "# 5 - Prétraitement des données / Data Preprocessing\n",
    "\n",
    "Le prétraitement des données est une tache cruciale en fouille de données. Cette étape nettoie et transforme les données brutes dans un format qui permet leur analyse, et leur utilisation avec des algorithmes de *machine learning*. En traitement des langages (natural language processing, NLP), la *tokenization* et le *stemming* sont des étapes cruciales. De plus, vous implémenterez une étape supplémentaire pour filtrer les mots sans importance.\n",
    "\n",
    "---\n",
    "\n",
    "Preprocessing is a crucial task in data mining. The objective is to clean and transform the raw data in a format that is better suited for data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well-known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "## 5.1 - Tokenization\n",
    "\n",
    "Cette étape permet de séparer un texte en séquence de *tokens* (= jetons, ici des mots, symboles ou ponctuation). Par example, la phrase *\"It's the student's notebook.\"* peut être séparé en liste de tokens de cette manière: [\"it\", \" 's\", \"the\", \"student\", \" 's\", \"notebook\", \".\"].\n",
    "\n",
    "---\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuation). For instance, the sentence *\"It's the student's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "### 5.1.1 - Question 2 (0.5 points) \n",
    "Implémentez la fonction suivante :\n",
    "\n",
    "- **tokenize_space** qui tokenize le texte à partir des blancs (espace, tabulation, nouvelle ligne). Ce tokenizer est naïf.\n",
    "- **tokenize_nltk** qui utilise le tokenizer par défaut de la librairie nltk (https://www.nltk.org/api/nltk.html).\n",
    "- **tokenize_space_punk** replace la ponctuation par des espaces puis tokenize les tokens qui sont séparés par des blancs (espace, tabulation, retour à la ligne).\n",
    "\n",
    "**Tous les tokenizers doivent mettre les tokens en minuscule.**\n",
    "\n",
    "---\n",
    "\n",
    "Implement the following functions: \n",
    "- **tokenize_space** tokenizes the tokens that are separated by whitespace (space, tab, newline). This is a naive tokenizer.\n",
    "- **tokenize_nltk** uses the default method of the nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "- **tokenize_space_punk** replaces the punctuation to space and then tokenizes the tokens that are separated by whitespace (space, tab, newline).\n",
    "\n",
    "**All tokenizers have to lowercase the tokens.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zM6Yq1pFt1H"
   },
   "outputs": [],
   "source": [
    "def tokenize_space(text):\n",
    "    \"\"\"\n",
    "    Tokenize the tokens that are separated by whitespace (space, tab, newline). \n",
    "    We consider that any tokenization was applied in the text when we use this tokenizer.\n",
    "    \n",
    "    For example: \"hello\\tworld of\\nNLP\" is split in ['hello', 'world', 'of', 'NLP']\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    \n",
    "    return text.lower().split()\n",
    "        \n",
    "def tokenize_nltk(text):\n",
    "    \"\"\"\n",
    "    This tokenizer uses the default function of nltk package (https://www.nltk.org/api/nltk.html) to tokenize the text.\n",
    "    \"\"\"\n",
    "    # return a list of tokens\n",
    "    \n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "def tokenize_space_punk(text):\n",
    "    \"\"\"\n",
    "    This tokenizer replaces punctuation to spaces and then tokenizes the tokens that are separated by whitespace (space, tab, newline).\n",
    "    \"\"\"\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation,' ')\n",
    "    \n",
    "    return text.lower().split()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "Using the tokenize_space tokenizer : ['hello', 'world', 'of', 'nlp,funky', '!stuff...', 'really', 'nice..', 'to', 'see']\n",
      "Using the tokenize_nltk tokenizer : ['hello', 'world', 'of', 'nlp', ',', 'funky', '!', 'stuff', '...', 'really', 'nice', '..', 'to', 'see']\n",
      "Using the tokenize_space_punk tokenizer : ['hello', 'world', 'of', 'nlp', 'funky', 'stuff', 'really', 'nice', 'to', 'see']\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To test the different tokenizers above\n",
    "\n",
    "text_example = \"hello\\tworld of\\nNLP\"\n",
    "print(tokenize_space(text_example)==['hello', 'world', 'of', 'nlp'])\n",
    "print(tokenize_nltk(text_example)==['hello', 'world', 'of', 'nlp'])\n",
    "print(tokenize_space_punk(text_example)==['hello', 'world', 'of', 'nlp'])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "text_example2 = \"hello\\tworld of\\nNLP,Funky !Stuff... really NICE.. TO SEE\"\n",
    "print(\"Using the tokenize_space tokenizer :\",tokenize_space(text_example2))\n",
    "print(\"Using the tokenize_nltk tokenizer :\",tokenize_nltk(text_example2))\n",
    "print(\"Using the tokenize_space_punk tokenizer :\",tokenize_space_punk(text_example2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpFMlhFTFt1J"
   },
   "source": [
    "## 5.2 - Suppression des mots vides / Stop words removal\n",
    "\n",
    "### 5.2.1 - Question 3 (0.5 point)\n",
    "\n",
    "Certains tokens sont sans importance pour la comparaison, car ils apparaissent dans la majorité des discussions. Les supprimer réduit la dimension du vecteur et accélère les calculs.\n",
    "\n",
    "Expliquez quels tokens sont sans importances pour la comparaison des discussions. Implémentez la fonction filter_tokens qui retire ces mots de la liste des tokens.\n",
    "\n",
    "---\n",
    "\n",
    "There are a set of tokens that are not significant to the similarity comparison since they appear in most of bug report pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper.\n",
    "\n",
    "Describe the tokens that can be removed without affecting the similarity comparison? Moreover, implement the function filter_tokens that removes these words from a list of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not', 'their', 'is', 'shouldn', \"aren't\", 'about', 'then', 'wasn', 'mustn', 'couldn', 've', 'once', 'am', 'where', 'those', 'out', 'other', 'all', \"you'd\", 'doesn', 'did', 'be', \"won't\", 'yours', 'was', 'a', 'does', 'on', 'and', 'him', 'y', 'ours', 'when', 'more', 'whom', \"haven't\", 'above', 'up', 'been', 'm', \"couldn't\", \"you're\", 'off', 'being', 'during', 'at', 'such', 'himself', \"that'll\", \"hasn't\", 'my', 'further', 'doing', \"you'll\", 'nor', 'this', 'itself', 't', 'i', 'haven', 'its', \"shouldn't\", 'our', 'any', \"wouldn't\", 'as', 'of', 'can', 'these', 'do', 'against', 'same', 'the', 'having', \"doesn't\", 'isn', 'ma', 'myself', 'they', 'while', 'mightn', 'won', 'how', \"mightn't\", 'from', 'in', 'weren', 'for', 'themselves', 'she', 'who', \"should've\", 'over', 's', 'below', 'few', 're', 'his', 'very', 'you', 'again', \"shan't\", 'don', \"isn't\", \"mustn't\", 'if', 'd', 'each', 'no', 'there', 'o', 'hasn', 'own', \"don't\", 'me', 'under', 'we', 'shan', 'had', 'needn', 'but', 'ain', 'through', 'too', 'only', \"weren't\", \"hadn't\", 'wouldn', 'before', 'so', 'your', 'or', 'herself', \"didn't\", 'why', \"it's\", 'than', 'into', 'down', 'an', 'here', 'just', 'are', 'both', 'yourself', 'because', 'were', 'some', 'most', 'what', 'should', 'her', 'which', 'have', 'with', 'after', 'by', 'aren', 'them', \"she's\", 'it', 'll', 'didn', 'theirs', 'between', \"wasn't\", 'now', 'has', 'hadn', \"you've\", 'that', 'ourselves', 'yourselves', 'will', 'until', \"needn't\", 'he', 'hers', 'to'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "usdvFGFHFt1J"
   },
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AITFRrPRFt1L"
   },
   "source": [
    "## 5.3 - Racinisation / Stemming\n",
    "\n",
    "La racinisation, ou *stemming* en anglais, est un procédé de transformation des flexions en leur radical ou racine. Par exemple, en anglais, la racinisation de \"fishing\", \"fished\" and \"fish\" donne \"fish\" (stem). \n",
    "\n",
    "---\n",
    "\n",
    "The process to convert tokens with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, the word \"fishing\", \"fished\" , \"fish\" and \"fisher\" are reduced to the stem \"fish\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAY10IY_Ft1L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'tri', 'differ', 'fish']\n",
      "['i', 'will', 'tri', 'onli', 'one', 'fish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "word1 = ['I', 'tried', 'different', 'fishes']\n",
    "\n",
    "print([ stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'will', 'tries', 'only', 'one', 'fishing']\n",
    "print([ stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnm9AJPPFt1M"
   },
   "source": [
    "### 5.3.1 - Question 4 (0.5 point) \n",
    "\n",
    "Expliquez comment et pourquoi le stemming est utile à notre système.\n",
    "\n",
    "---\n",
    "\n",
    "Explain how stemming can benefit our system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMHrsLC-Ft1N"
   },
   "source": [
    "We can notice that bug reporters when summarising or describing the bugs reported, they often reports similar expressions using different forms of some of the keywords of their report, so there might be a same meaning in the words of two different reporters but it won't count in the similarity calculation since it's not the same token. For example, the Bug 7526 is a duplicate of the bug 7799, the summary of the first one is \"Apprunner crashes on exit\" whereas the summary of the original bug is \"Crash on exit bugs\", so here without enabling stemming, our tokens set would contain \"crash\" and \"crashes\" where these two have the same meaning, so with stemming we expect to have more similarity between some of the documents when reducing some of the influential key words to their standard forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqwI0Y3OFt1O"
   },
   "source": [
    "# 6 - Représentation des données / Data Representation\n",
    "\n",
    "# 6.1 - Bag of Words\n",
    "\n",
    "De nombreux algorithmes demandent des entrées qui soient toutes de la même taille, ce qui n'est forcément le cas pour des types de données comme les textes, qui peuvent avoir un nombre variable de mots.  \n",
    "\n",
    "Par exemple, considérons la phrase 1, ”Board games are much better than video games” et la phrase 2, ”Monopoly is an awesome game!”. La table ci-dessous montre un exemple d'un moyen de représentation de ces deux phrases en utilisant une représentation fixe : \n",
    "\n",
    "|<i></i>     | an | are | ! | Monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 1 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Chaque colonne représente un mot du vocabulaire (de longueur 13), tandis que chaque ligne contient l'occurrence des mots dans une phrase. Ainsi, la valeur 2 à la position (1,7) est due au fait que le mot *\"games\"* apparaît deux fois dans la phrase 1. \n",
    "\n",
    "Ainsi, chaque ligne étant de longueur 13, on peut les utiliser comme vecteur pour représenter les phrases 1 et 2. Ainsi, c'est cette méthode que l'on appelle *Bag-of-Words* : c'est une représentation de documents par des vecteurs dont la dimension est égale à la taille du vocabulaire, et qui est construite en comptant le nombre d'occurrences de chaque mot. Ainsi, chaque token est ici associé à une dimension.\n",
    "\n",
    "---\n",
    "\n",
    "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two tweets: ”Board games are much better than video games” and ”Monopoly is an awesome game!”. These sentences are respectively named as Sentences 1 and 2. The table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e. an integer.\n",
    "\n",
    "<!-- Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "number of tweets. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *terrible* is rarer and documents that\n",
    "have this word are more likely to be negative. TF-IDF is a technique that overcomes the word frequency disadvantages. -->\n",
    "\n",
    "### 6.1.2 - Question 5 (2 points)\n",
    "\n",
    "\n",
    "Implémentez le Bag-of-Words en pondérant le vecteur par la fréquence de chaque mot.\n",
    "\n",
    "**Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) de scipy.**\n",
    "\n",
    "---\n",
    "\n",
    "Implement the bag-of-words model that weights the vector with the absolute word frequency.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g. scikit-learn). However, if you have a problem with memory size, you can use the scipy class [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_count_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, relates each token to a specific integer and  \n",
    "    transforms the text in a vector. Vectors are weighted using the token frequencies in the sentence.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\" \n",
    "    set_tokens = set() # It will contain the tokens we're dealing with\n",
    "    \n",
    "    for sentence in X:\n",
    "        set_tokens.update(sentence)\n",
    "        \n",
    "    tokens_dict = {val: id for id,val in enumerate(set_tokens)}  # Here we give an order of the tokens  \n",
    "    \n",
    "    data, indices, indptr = [], [], [0]\n",
    "    \n",
    "    for sentence in X:   \n",
    "        \n",
    "        for word in set(sentence):\n",
    "            \n",
    "            data.append(sentence.count(word))\n",
    "            indices.append(tokens_dict[word])\n",
    "        \n",
    "        indptr.append(indptr[-1]+len(set(sentence)))\n",
    "            \n",
    "    return csr_matrix((data, indices, indptr), shape=(len(X),len(set_tokens)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 1 1 0 1 0 0 1 1 0 1 0 1 0]]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 6.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens_example = [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "count_bow_mat = transform_count_bow(tokens_example)\n",
    "print(count_bow_mat.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zbp7EwyYFt1R"
   },
   "source": [
    "## 6.2 - TF-IDF\n",
    "\n",
    "L'utilisation de la fréquence d'apparition brute des mots, comme c'est le cas avec le bag-of-words, peut être problématique. En effet, peu de tokens auront une fréquence très élevée dans un document, et de ce fait, le poids de ces mots sera beaucoup plus grand que les autres, ce qui aura tendance à biaiser l'ensemble des poids. De plus, les mots qui apparaissent dans la plupart des documents n'aident pas à les discriminer. Par exemple, le mot \"*de*\" apparaît dans beaucoup de documents de la base de données, et pour autant, avoir ce mot en commun ne permet pas de conclure que des documents sont similaires. Au contraire, le mot \"*génial*\" est plus rare, mais les documents qui contiennent ce mot sont plus susceptibles d'être positifs. TF-IDF est donc une méthode qui permet de pallier à ce problème.\n",
    "\n",
    "TF-IDF pondère le vecteur en utilisant une fréquence de document inverse (IDF) et une fréquence de termes (TF).\n",
    "\n",
    "TF est l'information locale sur l'importance qu'a un mot dans un document donné, tandis que IDF mesure la capacité de discrimination des mots dans un jeu de données. \n",
    "\n",
    "L'IDF d'un mot se calcule de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{idf}_i = \\log\\left( \\frac{N}{\\text{df}_i} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "Avec $N$ le nombre de documents dans la base de données, et $\\text{df}_i$ le nombre de documents qui contiennent le mot $i$.\n",
    "\n",
    "Le nouveau poids $w_{ij}$ d'un mot $i$ dans un document $j$ peut ensuite être calculé de la façon suivante:\n",
    "\n",
    "\\begin{equation}\n",
    "  w_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i,\n",
    "\\end{equation}\n",
    "\n",
    "avec $\\text{tf}_{ij}$ la fréquence du mot $i$ dans le document $j$\n",
    "\n",
    "---\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document. Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides that, a set of words (including\n",
    "those with high frequency) appears in most of the documents and, therefore, they do not\n",
    "help to discriminate documents. For instance, the word *of* appears in a significant\n",
    "number of tweets. Thus, having the word *of* does not make\n",
    "documents more or less similar. However, the word *aeroplane* is rarer and documents that\n",
    "have this word are more likely to be similar. TF-IDF is a technique that overcomes the word frequency disadvantages.\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors. \n",
    "The following equation computes the word IDF:\n",
    "\\begin{equation}\n",
    "  idf_i = \\log\\left( \\frac{N}{df_i} \\right),\n",
    "\\end{equation}\n",
    "Where $N$ is the number of documents in the dataset $df_i$ is the number of documents that contain a word $i$.\n",
    "The new weight $w_{ij}$ of a word $i$ in a document $j$ using TF-IDF is computed as:\n",
    "\\begin{equation}\n",
    "  w_{ij} = tf_{ij} \\times idf_i,\n",
    "\\end{equation}\n",
    "Where $tf_{ij}$ is the term frequency of words $i$ in the document $j$.\n",
    "\n",
    "\n",
    "\n",
    "### 6.2.1 - Question 6 (2.5 points)\n",
    "\n",
    "Implémentez le bag-of-words avec la pondération de TF-IDF\n",
    "\n",
    "**Pour cette question, vous ne pouvez pas utiliser de librairie Python externe comme scikit-learn, hormis si vous avez des problèmes de mémoire, vous pouvez utiliser la classe [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html) de scipy.**\n",
    "\n",
    "---\n",
    "\n",
    "Implement a bag-of-words model that weights the vector using TF-IDF.\n",
    "\n",
    "**For this exercise, you cannot use any external python library (e.g. scikit-learn). However, if you have a problem with memory size, you can use the scipy class [sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tf_idf_bow(X):\n",
    "    \"\"\"\n",
    "    This method preprocesses the data using the pipeline object, calculates the IDF and TF and \n",
    "    transforms the text in vectors. Vectors are weighted using TF-IDF method.\n",
    "\n",
    "    X: document tokens. e.g: [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "\n",
    "    :return: vector representation of each document\n",
    "    \"\"\"\n",
    "    set_tokens = set() # It will contain the tokens we're dealing with\n",
    "    \n",
    "    for sentence in X:\n",
    "        set_tokens.update(sentence)\n",
    "        \n",
    "    tokens_dict = {val: i for i,val in enumerate(set_tokens)}  # Here we give an order of the tokens  \n",
    "\n",
    "    token_count, indices, indptr = [], [], [0]\n",
    "    \n",
    "    dict_document_count = dict((val,0) for val,i in tokens_dict.items())\n",
    "    \n",
    "    for sentence in X:   \n",
    "        \n",
    "        for word in set(sentence):\n",
    "            \n",
    "            token_count.append(sentence.count(word))\n",
    "            indices.append(tokens_dict[word])\n",
    "            dict_document_count[word] += 1             \n",
    "        \n",
    "        indptr.append(indptr[-1]+len(set(sentence)))\n",
    "    \n",
    "    idf = dict((i, np.log(len(X)/dict_document_count[val])) for val, i in tokens_dict.items())\n",
    "    data = [token_count[i]*idf[indices[i]] for i in range(len(indices))]   \n",
    "    \n",
    "\n",
    "    return csr_matrix((data, indices, indptr), shape=(len(X),len(set_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.09861229 0.         0.         0.         1.09861229\n",
      "  0.         0.         1.09861229 0.         0.         1.09861229\n",
      "  0.         1.09861229 0.         0.        ]\n",
      " [1.09861229 0.         0.         0.         0.         0.\n",
      "  0.         1.09861229 0.         0.         0.         0.\n",
      "  0.         0.         0.         1.09861229]\n",
      " [0.         0.         1.09861229 1.09861229 1.09861229 0.\n",
      "  1.09861229 0.         0.         1.09861229 1.09861229 0.\n",
      "  1.09861229 0.         1.09861229 0.        ]]\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens_example = [['I','will', 'be', 'back', '.'], ['Helllo', 'world', '!'], ['If', 'you', 'insist', 'on', 'using', 'a', 'damp', 'cloth']]\n",
    "tfidf_matrice= transform_tf_idf_bow(tokens_example)\n",
    "print(tfidf_matrice.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.60943791, 0.51082562, 0.22314355, 4.82831374],\n",
       "       [0.        , 0.        , 0.66943065, 0.        ],\n",
       "       [0.        , 0.        , 0.4462871 , 0.        ],\n",
       "       [0.        , 1.02165125, 0.22314355, 0.        ],\n",
       "       [0.        , 0.51082562, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "transform_tf_idf_bow([['A', 'B','C','D','D','D'],['A','A','A'],['A','A'],['A','B','B'],['B']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTPmhqzlFt1U"
   },
   "source": [
    "# 7 - Notre système / Our System\n",
    "\n",
    "## 7.1 - Question 7 (1 point)\n",
    "\n",
    "La *pipeline* est la séquence d'étapes de prétraitement des données qui transforme les données brutes dans un format qui permet leur analyse. Pour notre problème, implémentez un pipeline composé des étapes suivantes :\n",
    "\n",
    "1. Concatène le texte du résumé et description\n",
    "2. Tokenize le texte, retire les stop words et stem le tokens. \n",
    "3. Génère la représentation vectorielle avec transform_tf_idf_bow ou transform_count_bow.\n",
    "4. Encode données catégorielles (produit et composant) en entier \n",
    "\n",
    "La pipeline (fonction nlp_pipeline) prend en entrée la liste des rapports de bogues (liste des dictionnaires qui contiennent les informations des rapports), le type de tokenization, le type de vectorizer, un booléen qui active ou désactive la suppression des tokens inutiles et un booléen qui active ou désactive le stemming. La fonction nlp_pipeline retourne un tuple ($p$, $c$, $M$) :\n",
    "- $p$ est le vecteur qui contient l'identifiant des produits des rapports de bogues\n",
    "- $c$ est le vecteur qui contient l'identifiant des composants des rapports de bogues\n",
    "- $M$ est une matrice avec la représentation du texte.\n",
    "\n",
    "Le i-ème élément de $p$, $c$ et $M$ sont le produit, composant et la représentation du texte du i-ème rapport de bogue.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. For our problem, you have to implement a pipeline that:\n",
    "\n",
    "1. Concatenate the summary and description\n",
    "2. Perform the tokenization, stop word removal and stemming in textual data\n",
    "3. Generate the vector representation using transform_tf_idf_bow or transform_count_bow\n",
    "4. Encode the categorical data (the component and product) to integers\n",
    "\n",
    "\n",
    "The pipeline (nlp_pipeline function) receives a list of bug reports (dictionary that contains the report information), tokenization type, vectorizer type, a flag that enables or disable the insignificant token removal and a flag that turn stemming on or off. The nlp_pipeline function returns a tuple ($p$, $c$, $M$):\n",
    "- $p$ is a vector that contains the product values of the bug reports\n",
    "- $c$ is a vector that contains the component values of the bug reports\n",
    "- $M$ is a matrix with the text representation.\n",
    "\n",
    "The $i$-th element of $p$, $c$ and $M$ are the product, component and text representation of the $i$-th report in bug_reports, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeI39WT6Ft1U"
   },
   "outputs": [],
   "source": [
    "def nlp_pipeline(bug_reports, tokenization_type, vectorizer_type, enable_stop_words, enable_stemming):\n",
    "    \"\"\"\n",
    "    Preprocess and vectorize the threads.\n",
    "    \n",
    "    bug_reports: list of all bug reports([dict()]).\n",
    "    tokenization_type: two possible values \"space_tokenization\" and \"nltk_tokenization\".\n",
    "                            - space_tokenization: tokenize_space function is used to tokenize.\n",
    "                            - nltk_tokenization: tokenize_nltk function is used to tokenize.\n",
    "                            - space_punk_tokenization: tokenize_space_punk is used to tokenize.\n",
    "                            \n",
    "    vectorizer_type: two possible values \"count\" and \"tf_idf\".\n",
    "                            - count: use transform_count_bow to vectorize the text\n",
    "                            - tf_idf: use transform_tf_idf_bow to vectorize the text\n",
    "                            \n",
    "    enable_stop_words: Enables or disables the insignificant stop words removal\n",
    "    enable_stemming: Enables or disables steming\n",
    "    \n",
    "    return: tuple ($p$, $c$, $M$)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Concatenatig summary and description\n",
    "    summary_desc = [' '.join([report['summary'],report['description']]) for report in bug_reports]\n",
    "    \n",
    "    # Set of all the bug reports components and a set of all the bug reports products\n",
    "    components_set = set(report['component'] for  report in bug_reports)\n",
    "    products_set = set(report['product'] for report in bug_reports)\n",
    "    \n",
    "    # Tokenizing the new text\n",
    "    reports_cleaning = [tokenization_type(report_text) for report_text in summary_desc]\n",
    "           \n",
    "    if enable_stop_words: # Removing the stop words when the parameter takes True\n",
    "        reports_cleaning = [filter_tokens(tokens) for tokens in reports_cleaning]\n",
    "    \n",
    "    if enable_stemming: # Stemming when getting True for this parameter\n",
    "        reports_cleaning = [[stemmer.stem(word) for word in report] for report in reports_cleaning]  \n",
    "    \n",
    "    vectorized_reports = vectorizer_type(reports_cleaning) # The matrix with the vectors representation\n",
    "    components_dict = {val: id for id,val in enumerate(components_set)} # Encoding components to integer\n",
    "    products_dict = {val : id for id,val in enumerate(products_set)} # Encoding products to integer\n",
    "    \n",
    "    components_vals = [components_dict[report['component']] for report in bug_reports] # Vector containing components values\n",
    "    products_vals = [products_dict[report['product']] for report in bug_reports] # Vector containing products values\n",
    "    \n",
    "    return products_vals, components_vals, vectorized_reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BHMKa_yFt1W"
   },
   "source": [
    "## 7.2 - Question 8 (1 point)\n",
    "\n",
    "Implémentez la fonction rank qui retourne la liste des indices des rapports de bogues triée en fonction de la similarité entre les rapports de bug (candidat) et du nouveau rapport (requête). Vous utiliserez la fonction de similarité suivante pour comparer deux rapports :\n",
    "\n",
    "$$\n",
    "\\mathrm{SIM}(q,r) = w_1 * F_1(q,r) + w_2 * F_c(q,r) + w_3 * cos\\_sim(\\mathrm{txt}_q, \\mathrm{txt}_c),\n",
    "$$\n",
    "$$\n",
    " F_p(q,r) = \\begin{cases}\n",
    "    1 ,& \\text{si } p_q= p_r\\\\\n",
    "    0,              & \\text{autrement},\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    " F_p(q,r) = \\begin{cases}\n",
    "    1 ,& \\text{si } c_q = c_r\\\\\n",
    "    0,              & \\text{autrement},\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Où $c_q$ et $c_r$ sont les composants de la requête et du candidat,\n",
    " $p_q$ et $p_r$ sont les produits de la requête et du candidat,\n",
    " $\\mathrm{txt}_q$ et $\\mathrm{txt}_c$ sont les représentations du texte de la requête et du candida. Les paramètres \n",
    " w_1, w_2 et w_3 doivent être réglés.\n",
    " \n",
    "\n",
    "**Pour cette question, la requête doit être retirée de la liste triée (sortie de la fonction rank).**\n",
    "\n",
    "*Pour de meilleures performances, vous pouvez utiliser la fonction cos d'une libraire (par exemple [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)) pour calculer la similarité. Il est préférable de faire des opérations matricielles.*\n",
    "\n",
    "---\n",
    "\n",
    "Implement the function rank that returns a list of reports indexes sorted by similarity of the bug reports (candidates) and new bug report (query). We use the following similarity function to compare two bug reports:\n",
    "\n",
    "$$\n",
    "\\mathrm{SIM}(q,r) = w_1 * F_1(q,r) + w_2 * F_c(q,r) + w_3 * cos\\_sim(\\mathrm{txt}_q, \\mathrm{txt}_c),\n",
    "$$\n",
    "$$\n",
    " F_p(q,r) = \\begin{cases}\n",
    "    1 ,& \\text{if } p_q= p_r\\\\\n",
    "    0,              & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    " F_p(q,r) = \\begin{cases}\n",
    "    1 ,& \\text{if } c_q = c_r\\\\\n",
    "    0,              & \\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "Where $c_q$ and $c_r$ are the query and candidate components,\n",
    " $p_q$ and $p_r$ are the query and candidate products,\n",
    " $\\mathrm{txt}_q$ and $\\mathrm{txt}_c$ are the query and candidate textual representations, respectively. The parameters \n",
    " w_1, w_2 and w_3 are to be tuned.\n",
    " \n",
    "\n",
    "**In this question, the query has to  be removed in the sorted list (rank output).**\n",
    "\n",
    "*For better performance, you can use the cosine similarity from a library (e.g. [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)). Also, we recommend performing matrix operations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.22 s\n",
      "Wall time: 5.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pro, comp, K = nlp_pipeline(reports1, tokenize_space, transform_tf_idf_bow, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity from scratch\n",
    "\n",
    "def cos_sim(A,B):\n",
    "    prod = np.dot(A, B.transpose()) # prod[i,j] equals the scalar product between the ith row of A and jth row of B\n",
    "    dist_A = np.sqrt(np.sum(np.square(A), axis=1)) # dist_A[i] is the distance of the ith row of A\n",
    "    dist_B = np.sqrt(np.sum(np.square(A), axis=1)) # dist_B[i] is the distance of the ith row of B\n",
    "#     prod_dist = np.outer(dist_A, dist_B) # prod_dist[i,j] return the product of the distance of the ith row in A with the distance of the jth row in B\n",
    "    return np.divide(prod, np.outer(dist_A, dist_B)) # returns the cosine similarity betwenen the two matrices, output[i,j] is the cosine similarity between the ith row in A and jth row in B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2f37FYL8Ft1W"
   },
   "outputs": [],
   "source": [
    "def rank(query_idx, p, c, M, w1, w2, w3):\n",
    "    \"\"\"\n",
    "    Return a list of reports indexes sorted by similarity of the bug reports (candidates) and new bug report (query)\n",
    "    Cosine similarity is used to compare bug reports. \n",
    "    \n",
    "    query_idx: query indexes\n",
    "    p: product values of all bug reports (list)\n",
    "    c: component values of all bug reports  (list)\n",
    "    M: textual data representation of all bug reports  (Matrix)\n",
    "    \n",
    "    w1: parameter that controls the impact of the product\n",
    "    w2: parameter  that controls the impact of the component\n",
    "    w3: parameter  that controls the impact of textual similarity\n",
    "    \n",
    "    return: ranked list of indexes. \n",
    "    \"\"\"\n",
    "    if w3 == 0:\n",
    "        cos_query = 0\n",
    "    else:\n",
    "        cos_query = cosine_similarity(M,M[query_idx:query_idx+1]) # It returns a numpy array of a (len(p),1) shape of similarities between the query and other bug reports.\n",
    "    \n",
    "    if w1 == 0:\n",
    "        Fp = 0\n",
    "    else:\n",
    "        Fp = np.array([int(p[query_idx]==j) for j in p]).reshape(len(p),1) # It returns a numpy array of a (len(p),1) shape, of comparison between query product and other bug reports products.\n",
    "        \n",
    "    if w2 == 0:\n",
    "        Fc = 0\n",
    "    else:\n",
    "        Fc = np.array([int(c[query_idx]==j) for j in c]).reshape(len(c),1) # It returns a numpy array of a (len(c),1) shape, of comparison between query component and other bug reports components.\n",
    "    \n",
    "    sim_query = w1*Fp + w2*Fc + w3*cos_query\n",
    "    sim_query_arr = list(sim_query[:,0])\n",
    "    \n",
    "    sorted_indexes = list(list(zip(*sorted([(val, i) for i, val in enumerate(sim_query_arr)], reverse = True)))[1]) # It returns the list of indexes sorted\n",
    "    sorted_indexes.remove(query_idx)\n",
    "    \n",
    "    return sorted_indexes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[153, 7824, 1181, 547, 7186, 6881, 4913, 3665, 8039, 1393, 372, 347, 3862, 7326, 7813, 399, 7076, 487, 611, 2379, 2312, 344, 2852, 4962, 1529, 2042, 380, 2853, 3796, 1280, 7098, 7931, 9474, 8225, 6903, 1349, 2346, 667, 6936, 444, 4739, 7392, 6826, 6914, 2072, 9377, 2491, 1360, 2887, 8355, 1305, 6925, 1382, 7488, 3632, 1992, 778, 8593, 7065, 700, 9303, 8834, 7770, 7413, 7381, 317, 7575, 1436, 9965, 3251, 7402, 602, 4136, 1759, 1682, 2267, 3928, 2266, 9410, 1404, 987, 450, 1671, 4717, 1748, 4201, 1346, 2876, 4125, 5146, 1760, 9248, 8582, 2762, 406, 407, 6444, 2215, 131, 2933, 305, 2624, 360, 3922, 1332, 1308, 7114, 193, 4182, 294, 3775, 186, 5258, 3411, 1451, 2632, 6217, 3976, 9668, 4997, 2772, 3022, 4332, 3033, 1581, 5777, 395, 1411, 1931, 1780, 1712, 1613, 9032, 2736, 629, 1204, 2302, 1162, 5531, 7493, 3664, 7515, 8203, 5684, 1330, 1889, 3354, 3653, 5836, 1073, 1533, 1157, 5661, 5605, 946, 577, 4042, 7502, 835, 498, 1005, 5725, 7477, 2111, 944, 316, 8255, 2335, 1240, 838, 2297, 4060, 2476, 3751, 866, 1543, 1542, 8121, 8120, 6284, 3101, 3726, 1372, 4408, 5471, 4092, 7750, 7172, 1967, 5282, 2692, 6989, 665, 1003, 4026, 2342, 6030, 1133, 1000, 2600, 831, 1107, 5911, 1267, 4560, 2000, 4134, 2633, 2467, 9046, 730, 3778, 7543, 1425, 3666, 8962, 8137, 9445, 7525, 2839, 2711, 951, 1677, 2777, 892, 2975, 1060, 4535, 6783, 5485, 3422, 1878, 900, 7653, 2788, 2558, 3548, 1485, 2620, 948, 4820, 8351, 4167, 1704, 635, 1019, 2818, 1930, 9193, 222, 2622, 4781, 6892, 510, 2546, 2847, 8051, 3933, 1015, 238, 1905, 3911, 9490, 1166, 7614, 2363, 8300, 1852, 584, 1282, 7149, 548, 954, 4848, 3748, 2638, 591, 940, 3738, 4058, 499, 1886, 5817, 33, 5233, 8040, 2334, 7452, 2909, 6774, 1632, 6755, 8910, 2270, 1968, 5199, 5678, 3631, 9294, 871, 2065, 8171, 2493, 3856, 1223, 4309, 8159, 1979, 4451, 7053, 1914, 509, 960, 1491, 1112, 9277, 1636, 4711, 8670, 4674, 1199, 5127, 3432, 952, 945, 8006, 3429, 8403, 3714, 1835, 2502, 2690, 2701, 6863, 2611, 4319, 9988, 5980, 4147, 477, 4961, 6728, 7693, 3813, 544, 503, 5712, 7313, 3030, 4024, 6630, 3048, 4755, 7467, 8356, 4972, 6821, 378, 3457, 36, 1608, 7131, 1822, 2657, 5627, 7468, 997, 1803, 2368, 87, 2784, 9234, 2910, 3459, 8629, 7022, 355, 2668, 3218, 1036, 9662, 5869, 6957, 7886, 1171, 8813, 2078, 3036, 672, 8388, 6868, 1807, 1172, 4101, 2543, 3817, 4303, 6618, 5821, 1729, 2390, 40, 7954, 1105, 6631, 3389, 3207, 6866, 3196, 856, 2129, 7011, 7801, 9507, 2186, 3298, 7153, 3063, 2616, 6635, 2568, 7534, 3720, 2305, 7586, 6349, 1316, 3646, 1387, 1120, 6350, 2819, 6031, 2098, 7249, 5739, 411, 1306, 4888, 7388, 5022, 4467, 3654, 4258, 9227, 680, 8831, 9050, 4852, 6351, 1938, 930, 8816, 4157, 3055, 7943, 108, 1274, 4789, 632, 3049, 2101, 2438, 998, 2477, 600, 2389, 1385, 5498, 4355, 8404, 4160, 9093, 3731, 8980, 2158, 1128, 2586, 8470, 2301, 9547, 4307, 8361, 7884, 1640, 6003, 6862, 1483, 1659, 967, 3603, 4088, 3, 2054, 9920, 3620, 82, 3668, 8110, 1258, 5909, 9100, 7157, 2387, 451, 1035, 986, 5802, 6661, 2076, 689, 2986, 7372, 486, 5615, 4883, 8761, 1033, 999, 8414, 7651, 8702, 369, 917, 8624, 3981, 5332, 6968, 1273, 7737, 9545, 409, 8273, 9297, 4245, 6676, 3523, 1368, 4553, 9896, 6280, 1939, 4411, 2385, 1377, 3959, 446, 445, 8793, 5027, 1954, 366, 8428, 2053, 1130, 8162, 5816, 3240, 5703, 2069, 4428, 9401, 7113, 4278, 3625, 525, 8135, 5735, 7603, 9217, 5135, 4845, 5770, 2254, 5125, 990, 1847, 5, 2789, 1770, 4129, 7410, 7225, 5773, 7346, 2303, 966, 722, 2947, 5896, 3677, 6833, 2955, 3335, 6325, 4238, 8282, 7872, 5623, 5187, 8579, 2898, 9544, 2854, 1540, 7715, 2589, 9726, 2391, 8933, 7572, 1423, 4421, 2059, 1493, 542, 698, 1692, 739, 687, 3414, 8237, 8046, 2300, 1129, 7376, 2679, 5050, 7670, 2984, 6584, 3255, 6864, 3557, 4814, 1327, 7438, 1367, 4429, 336, 142, 6684, 9846, 8595, 2579, 1388, 7683, 7314, 2799, 6859, 7000, 3947, 6949, 512, 2165, 2024, 2423, 6340, 6437, 436, 1915, 949, 1897, 4694, 5833, 2093, 7105, 5757, 6820, 4071, 388, 2184, 1791, 2170, 4733, 6014, 2838, 4559, 2588, 1405, 9890, 4657, 9244, 2397, 4983, 4821, 8181, 5705, 3764, 4304, 7960, 6343, 8871, 1007, 5862, 5253, 371, 6292, 2439, 971, 2717, 3906, 1255, 8008, 4097, 6042, 959, 2708, 592, 3386, 8074, 8857, 7702, 8218, 927, 4376, 886, 6354, 1839, 1772, 857, 2595, 1344, 1070, 6021, 6740, 7420, 6727, 2166, 6222, 9964, 745, 734, 692, 1283, 9039, 8781, 2950, 2015, 640, 4149, 693, 2361, 2524, 3043, 8415, 1935, 3054, 4707, 6683, 5091, 7909, 5915, 8011, 6849, 4240, 3095, 4804, 8928, 4796, 7152, 5989, 341, 1263, 7133, 3109, 588, 6690, 6633, 1173, 4168, 1188, 7280, 4944, 7147, 3825, 9222, 4793, 413, 6848, 6283, 3249, 1507, 6339, 1329, 1781, 2929, 1848, 8753, 3546, 7300, 4924, 8117, 2801, 2800, 4196, 744, 2901, 1235, 1707, 6874, 859, 7021, 6526, 2050, 3247, 2805, 3061, 2623, 58, 9722, 1078, 410, 1989, 2320, 2140, 2393, 7120, 8651, 481, 1715, 569, 4383, 4306, 219, 788, 384, 9575, 8997, 9564, 595, 1901, 2417, 4430, 552, 5842, 1158, 9835, 2107, 9256, 2951, 8757, 2428, 8992, 3771, 7176, 7510, 562, 9205, 1371, 738, 2905, 852, 5456, 6009, 8413, 3883, 1390, 8335, 5309, 7437, 379, 5887, 5074, 599, 6870, 3241, 5435, 8860, 875, 2722, 9130, 2086, 4321, 7945, 6872, 7046, 6658, 7720, 365, 1972, 1138, 3427, 6887, 2022, 3403, 9228, 9226, 7170, 382, 1357, 1375, 2058, 9220, 471, 4728, 9986, 8514, 9856, 950, 872, 1031, 1940, 2227, 7640, 4364, 3999, 9152, 5925, 1418, 3034, 6067, 5395, 7478, 1754, 2714, 2414, 1649, 9489, 5728, 332, 7523, 5715, 2842, 1638, 2796, 1415, 8474, 4036, 6844, 7521, 2357, 9532, 3257, 5900, 1627, 2113, 5483, 334, 1719, 439, 8157, 2176, 8106, 5872, 6353, 1189, 1532, 5313, 340, 5582, 2967, 6576, 9659, 6877, 7104, 1466, 374, 7799, 5835, 6132, 1109, 7148, 412, 570, 1294, 5025, 6213, 938, 8285, 2424, 1363, 4826, 8867, 6935, 7400, 3173, 537, 4837, 4706, 2160, 8616, 3185, 8623, 868, 922, 1708, 4150, 2696, 9954, 4958, 8030, 6462, 4607, 6242, 6517, 753, 2907, 2418, 4746, 756, 2904, 4179, 7851, 8107, 7974, 8986, 2051, 7368, 4815, 9054, 3044, 2252, 2993, 6090, 7585, 7235, 2767, 434, 2277, 9431, 7834, 6295, 1134, 1965, 5255, 9798, 2260, 9192, 2532, 4098, 1666, 1376, 586, 558, 8253, 1687, 9707, 6342, 2806, 6267, 2437, 1362, 7548, 1234, 3208, 1794, 4082, 1434, 1639, 8889, 5521, 1400, 3851, 3285, 6858, 2262, 8533, 8289, 574, 6860, 8911, 2155, 1482, 3859, 8226, 2455, 3779, 9496, 5036, 9171, 5920, 660, 4127, 4772, 614, 662, 1616, 4478, 6251, 4724, 2004, 2513, 8767, 511, 342, 7944, 1764, 4549, 564, 6818, 1883, 3671, 425, 9699, 878, 5230, 6981, 1378, 7103, 3215, 983, 1323, 1860, 4934, 2883, 2066, 7073, 9795, 4433, 5188, 3497, 7429, 464, 1410, 4212, 2047, 320, 6051, 4908, 2770, 2584, 2645, 4653, 2572, 3243, 466, 2894, 7337, 4190, 5848, 3682, 1256, 8900, 9868, 6978, 7056, 4038, 6045, 919, 3452, 2742, 5813, 8146, 3440, 7629, 728, 9207, 4117, 8878, 720, 3248, 5969, 5130, 2308, 2206, 329, 5789, 5543, 1927, 8922, 2096, 350, 6287, 1786, 2871, 1014, 4891, 3104, 1518, 701, 2314, 3322, 931, 351, 218, 2519, 2205, 2063, 2768, 1700, 6503, 9185, 792, 2395, 1288, 5760, 3442, 1475, 933, 2660, 1467, 3118, 2229, 8913, 981, 8633, 4670, 6138, 5410, 3002, 2698, 9048, 1150, 3937, 3433, 4906, 506, 1870, 9350, 6432, 4865, 357, 6328, 4812, 9605, 5341, 8636, 2942, 4573, 842, 8377, 3123, 345, 1076, 1913, 1925, 955, 1354, 749, 7087, 5685, 814, 2666, 8141, 8329, 7537, 2249, 1012, 1383, 324, 7514, 1556, 6341, 9717, 4922, 1673, 7134, 501, 6082, 367, 9448, 1893, 1896, 1074, 6017, 476, 2430, 5393, 9993, 3709, 767, 540, 2218, 9417, 3406, 9912, 7226, 2931, 7435, 437, 7175, 3170, 1427, 5660, 2238, 9019, 6706, 3113, 5268, 315, 6672, 7375, 6653, 679, 4053, 3418, 1248, 1769, 5167, 383, 5859, 5390, 6565, 6089, 8746, 1114, 2565, 9526, 622, 3535, 9235, 3952, 8622, 1062, 9319, 1277, 903, 7802, 4489, 1955, 2837, 5155, 9194, 9281, 2920, 676, 4952, 9884, 3651, 4264, 8768, 1183, 2468, 885, 2900, 3462, 1135, 8520, 2425, 9325, 6020, 325, 1492, 5921, 9681, 2036, 6687, 1975, 8648, 2547, 8978, 727, 725, 1952, 3323, 1924, 2436, 1159, 5868, 4989, 2118, 1412, 7844, 7748, 1261, 5657, 1098, 5544, 401, 3107, 240, 6382, 1942, 349, 2048, 4846, 6345, 1562, 4611, 5790, 3886, 9961, 8823, 6871, 2517, 1370, 4673, 7378, 3828, 2208, 5899, 3953, 1490, 1580, 381, 3320, 1990, 7607, 6543, 1034, 1152, 6762, 956, 6010, 2173, 7727, 1146, 8354, 3559, 414, 1597, 1217, 6845, 3958, 3286, 6671, 2171, 4902, 2749, 4918, 2610, 536, 5645, 4567, 9238, 6869, 678, 4409, 20, 292, 8416, 2203, 589, 4761, 9086, 1221, 9397, 364, 1079, 8227, 7424, 448, 6795, 9065, 8742, 8073, 862, 5922, 963, 6154, 1880, 1825, 1338, 3387, 4601, 825, 5454, 7255, 2175, 6593, 4818, 7604, 8861, 390, 4713, 7854, 5668, 331, 1974, 1358, 3826, 514, 964, 2731, 7428, 2987, 613, 1401, 4333, 5013, 5651, 2068, 897, 7817, 2990, 580, 2194, 3589, 1693, 2803, 6623, 363, 8456, 925, 4663, 6459, 836, 2652, 5808, 6235, 6841, 7151, 8164, 554, 9742, 2371, 1192, 7158, 346, 5368, 3439, 9163, 9366, 475, 3463, 158, 8133, 2636, 5182, 8599, 337, 4255, 9441, 615, 7456, 5277, 2061, 2162, 2526, 7791, 2998, 9298, 6780, 2535, 1894, 9045, 1810, 1948, 2964, 2409, 8655, 1462, 452, 9270, 5540, 2304, 7132, 5407, 1552, 8704, 9259, 3967, 8095, 2373, 1983, 4379, 3450, 585, 9952, 7445, 1438, 3742, 3793, 6492, 2392, 819, 2862, 521, 7197, 387, 7564, 4310, 4996, 4919, 3629, 7808, 7038, 261, 846, 8812, 2388, 348, 2001, 8402, 9447, 9987, 5753, 5771, 8853, 110, 2017, 2637, 896, 4223, 8712, 594, 6281, 458, 962, 2280, 993, 2075, 3842, 6616, 2406, 566, 500, 1156, 3533, 2693, 4398, 4161, 1976, 1902, 1595, 2276, 686, 8915, 2159, 3556, 2237, 3378, 4226, 7852, 590, 1670, 3961, 2239, 1210, 793, 92, 8807, 8298, 8777, 6737, 943, 690, 120, 1213, 1511, 1814, 760, 3100, 4805, 5958, 8625, 5947, 368, 1271, 1539, 1541, 1512, 1515, 1513, 5794, 1505, 1547, 1544, 1510, 1509, 1546, 1545, 1514, 1508, 1506, 8673, 2808, 2765, 8907, 1246, 1197, 5028, 1503, 5696, 2908, 1709, 1504, 553, 2421, 3835, 2791, 5174, 7444, 2923, 8322, 567, 1986, 377, 5505, 5015, 3062, 9828, 669, 8952, 3571, 7780, 5183, 1426, 523, 6946, 8557, 2548, 790, 5353, 5691, 1441, 1594, 2922, 583, 9021, 2766, 497, 2646, 969, 4838, 8626, 961, 3273, 2034, 1328, 8914, 2023, 2755, 2744, 1440, 1765, 9518, 2753, 3850, 5686, 2855, 1871, 9830, 1059, 3916, 3263, 2739, 391, 7930, 9834, 1843, 3524, 587, 3970, 9827, 1726, 330, 575, 9811, 9141, 2790, 4016, 2224, 616, 2225, 787, 2135, 3754, 8888, 5457, 474, 8062, 8659, 8399, 7725, 5021, 3204, 2994, 2981, 2540, 1619, 1026, 483, 84, 66, 2786, 4702, 2809, 1040, 9036, 7230, 354, 8376, 70, 5493, 4700, 7751, 5538, 224, 3479, 8525, 3940, 3690, 4340, 8839, 8830, 6062, 6227, 7675, 6666, 3316, 4987, 83, 3315, 3699, 4709, 5108, 5496, 5866, 3811, 7498, 7497, 8646, 7357, 91, 3616, 1751, 5671, 3808, 1783, 3919, 9991, 4664, 7161, 4685, 4459, 9337, 5202, 3921, 8614, 3641, 9555, 3075, 1304, 5960, 9230, 3920, 3182, 3029, 8275, 6460, 9866, 4847, 12, 9028, 1006, 9087, 7961, 3963, 5067, 6007, 3628, 6647, 39, 5004, 9712, 6547, 9333, 9326, 2577, 314, 9089, 184, 4723, 7746, 9413, 8951, 3927, 1307, 9345, 5259, 5756, 8701, 2549, 7767, 3703, 9001, 5100, 3936, 8446, 9880, 6164, 7963, 5459, 3788, 1494, 18, 1721, 5979, 1341, 2318, 5699, 8024, 4672, 4745, 2918, 7869, 5237, 4719, 8151, 9049, 5162, 4112, 4403, 5408, 7757, 8658, 7600, 7968, 6422, 5526, 7863, 5758, 1676, 6443, 8644, 8440, 5975, 7918, 6228, 3747, 5917, 2872, 5068, 4537, 1602, 2291, 8984, 3943, 7058, 3415, 1061, 7866, 5308, 3573, 2979, 3581, 9881, 891, 5895, 4151, 6419, 914, 5626, 3514, 7171, 1216, 6279, 2946, 1790, 7736, 5040, 286, 1834, 732, 5534, 4402, 3907, 4752, 2890, 9463, 1165, 5675, 8175, 1778, 9286, 229, 6606, 8338, 3151, 1447, 3015, 5158, 5156, 9479, 8840, 8733, 1833, 3292, 3905, 3755, 2473, 5165, 144, 246, 3858, 7168, 6264, 9203, 4374, 5160, 3016, 7617, 8947, 8885, 1194, 8431, 3496, 1027, 4819, 3234, 1373, 9395, 5680, 7800, 1933, 3484, 2192, 581, 52, 6238, 4612, 4463, 7177, 5112, 5279, 6961, 6112, 1052, 8032, 7610, 9125, 8178, 3650, 7405, 5704, 5121, 9750, 4867, 6226, 4195, 4750, 3011, 9648, 6091, 6777, 9739, 9138, 3951, 1985, 7309, 3284, 9675, 3301, 1987, 4608, 1923, 6102, 9378, 7285, 1384, 4839, 1251, 2032, 528, 4121, 4581, 4314, 9687, 3794, 6244, 1477, 4665, 691, 5720, 1982, 4432, 1047, 3097, 4722, 652, 5592, 579, 5263, 1237, 3152, 4941, 1936, 2590, 4302, 1095, 1184, 3260, 6002, 2945, 2902, 1763, 6519, 2969, 326, 7613, 1958, 8507, 3157, 5007, 4029, 4354, 8649, 488, 7308, 433, 4131, 3230, 4942, 4831, 34, 3092, 3480, 3503, 2970, 3799, 282, 7611, 6835, 4477, 185, 1229, 4401, 8923, 9245, 7956, 3203, 5853, 2552, 4692, 8421, 5830, 5035, 8858, 8475, 429, 4524, 3146, 5924, 9247, 5784, 5621, 8706, 5807, 4823, 2712, 7135, 2747, 1478, 4763, 883, 3149, 2930, 2607, 752, 4491, 1116, 1879, 9996, 2601, 7486, 3752, 5123, 3357, 4938, 460, 8346, 7448, 1550, 5749, 3225, 8694, 7595, 9706, 2810, 3892, 75, 296, 6664, 695, 7762, 385, 3186, 6258, 5223, 4551, 2336, 5755, 3849, 8862, 1629, 7030, 4833, 8187, 3374, 6765, 2559, 4710, 6404, 8277, 7662, 3925, 7074, 7266, 4773, 5273, 8981, 5805, 5891, 2338, 2944, 5998, 2655, 8400, 1973, 9682, 8248, 3052, 9037, 6649, 2582, 7240, 7419, 1214, 1417, 3887, 4872, 4973, 2554, 4204, 1586, 3153, 8048, 1268, 5317, 3861, 9962, 3993, 996, 9051, 8311, 5350, 8785, 9134, 1465, 7302, 8427, 8902, 5006, 7703, 9925, 1674, 5806, 6428, 6708, 8929, 7068, 2274, 9, 7753, 2583, 9381, 7962, 9460, 6751, 6865, 6776, 68, 9253, 6375, 8828, 8912, 7182, 7842, 1260, 2104, 2825, 4795, 3277, 8880, 6059, 5010, 2594, 9126, 6956, 8185, 250, 1460, 5398, 1437, 9990, 2496, 4519, 4556, 8426, 8358, 1442, 2840, 2972, 4787, 4086, 5571, 8245, 1117, 8466, 7312, 9221, 8485, 7818, 1837, 3536, 195, 2798, 6809, 9383, 417, 4585, 4025, 6827, 5648, 5550, 649, 5470, 4480, 5666, 4341, 7915, 5673, 3789, 5385, 9058, 5546, 3798, 4523, 4904, 4903, 1818, 4481, 8642, 6005, 9071, 7540, 8760, 5520, 1805, 8843, 6811, 4158, 5373, 220, 1706, 1017, 1168, 3741, 8366, 47, 2506, 3675, 9604, 6331, 5825, 1063, 3510, 8113, 3987, 5518, 5440, 3740, 9656, 5292, 6682, 7032, 5894, 4776, 8829, 1077, 5580, 9792, 7580, 5374, 4446, 5970, 659, 9444, 3213, 8063, 7986, 2073, 9718, 9491, 6425, 5366, 904, 6142, 6607, 2603, 5681, 6958, 6878, 1361, 9702, 2534, 7382, 4850, 817, 419, 1245, 9799, 9038, 1386, 7694, 4166, 8271, 2407, 1071, 881, 8331, 6689, 5791, 1085, 2597, 8572, 5877, 7332, 4771, 4568, 7345, 6423, 9470, 8236, 8682, 8960, 8368, 5986, 1262, 8315, 285, 9836, 1702, 9935, 7189, 2081, 5722, 2551, 4047, 6323, 9785, 1978, 6850, 8963, 9774, 5591, 4186, 3197, 4085, 7377, 7870, 115, 9375, 4712, 4533, 4228, 9767, 3881, 2719, 8795, 6369, 1910, 3025, 7905, 5256, 6333, 424, 811, 8787, 4346, 9921, 2378, 9542, 4647, 3236, 9278, 8023, 4998, 6481, 2602, 5630, 7278, 6851, 5500, 44, 8824, 8905, 4438, 2561, 6571, 7079, 4966, 3079, 9878, 7966, 8798, 5533, 1964, 2656, 9983, 5319, 9275, 5549, 2574, 4770, 4447, 6329, 8291, 2283, 9209, 4313, 9895, 5864, 8385, 1259, 6518, 6320, 4404, 991, 9488, 7457, 3602, 5497, 5579, 4856, 8010, 6812, 5708, 5113, 1051, 9117, 300, 7609, 7539, 5124, 9664, 6624, 2578, 4811, 4331, 1747, 4141, 7384, 8513, 6643, 7701, 3216, 3931, 1937, 9949, 2677, 3884, 1736, 4124, 5377, 1589, 6704, 3021, 3590, 7110, 1289, 106, 5594, 8855, 8223, 7932, 5082, 9290, 151, 3168, 1322, 1838, 2893, 8692, 1583, 2659, 5642, 4406, 5933, 1963, 6389, 5333, 1406, 9716, 8327, 7067, 992, 9825, 628, 2802, 422, 98, 6309, 2515, 430, 6502, 5832, 8064, 8665, 3210, 6499, 8465, 898, 6920, 5717, 2478, 3746, 4198, 3857, 421, 7174, 5768, 976, 8216, 4705, 4360, 3786, 8854, 1025, 4342, 7163, 1053, 59, 8115, 4667, 55, 6540, 9971, 3035, 6918, 8481, 1756, 4100, 3686, 7241, 849, 7213, 6330, 8680, 7102, 7920, 3914, 9967, 8360, 5371, 8070, 2754, 658, 2290, 9953, 5982, 4386, 7288, 4087, 2647, 7236, 7004, 8870, 6554, 8779, 8939, 8029, 8957, 3579, 5120, 8806, 9018, 4945, 1776, 5616, 3138, 4861, 1766, 3913, 4375, 8406, 8405, 7744, 9459, 1413, 741, 9394, 9267, 4532, 8320, 7893, 4898, 2033, 5250, 4013, 4472, 2629, 1155, 6927, 5192, 1290, 5337, 7796, 8570, 4632, 5248, 8552, 8863, 2968, 4052, 6778, 2570, 7732, 3112, 769, 4561, 2983, 352, 762, 2604, 7273, 6153, 6230, 132, 8903, 2948, 3258, 4577, 4843, 3471, 5278, 8242, 1582, 3898, 5733, 7499, 5203, 5328, 8972, 8081, 2651, 7492, 9016, 4345, 4172, 5463, 3678, 8019, 4392, 761, 7503, 7882, 9657, 8116, 4322, 3125, 4363, 74, 5595, 5741, 9336, 9689, 5988, 6454, 1099, 3774, 3419, 5293, 9663, 3451, 9156, 1548, 3908, 8169, 8631, 8454, 5369, 1001, 6656, 5731, 1381, 4009, 408, 3013, 2445, 3809, 9768, 7597, 4137, 7579, 675, 9133, 6260, 8317, 8283, 531, 6586, 6884, 3805, 4496, 2247, 187, 5320, 6085, 2198, 9122, 4152, 3148, 8517, 5516, 8668, 7231, 8560, 5023, 5502, 6527, 7563, 7941, 8841, 8732, 4976, 2527, 2340, 2606, 3436, 17, 8318, 1912, 2284, 5364, 8261, 1270, 5545, 3053, 5077, 7996, 5892, 8365, 6438, 708, 3245, 8868, 5231, 1342, 7590, 1391, 5824, 4187, 4191, 283, 847, 2571, 5643, 8792, 5115, 9775, 5110, 7208, 8192, 9072, 5126, 7549, 7541, 9951, 7254, 9282, 9443, 6861, 5499, 3926, 9938, 3274, 5089, 2472, 64, 8721, 8153, 5823, 2136, 2870, 7476, 4800, 4909, 1752, 5224, 2279, 7877, 4095, 4530, 7334, 5672, 2147, 3019, 6516, 5434, 8503, 4625, 8013, 5037, 3476, 6305, 3978, 9751, 4099, 9509, 139, 8111, 8363, 1909, 6720, 7183, 5799, 3757, 773, 3163, 9766, 5386, 6670, 4119, 5318, 2386, 6597, 5205, 8935, 2475, 4645, 6986, 2035, 4503, 7500, 3444, 9066, 105, 4028, 4785, 3770, 3708, 3707, 2480, 4122, 8002, 5024, 746, 8805, 3069, 6374, 915, 1489, 7043, 5700, 6299, 2243, 7706, 2498, 4390, 7108, 8927, 2219, 4008, 1761, 5261, 3566, 1737, 2163, 6319, 6511, 869, 3268, 6719, 5359, 4140, 4516, 3511, 4217, 4974, 4084, 4879, 3990, 4762, 4552, 3404, 5468, 8667, 4443, 457, 7967, 2609, 3064, 259, 6221, 3663, 7671, 7583, 8677, 5857, 6713, 9559, 5734, 4994, 5815, 1574, 5042, 685, 3712, 8637, 8604, 4457, 6352, 2295, 6721, 8497, 7007, 3280, 2460, 9067, 4216, 3059, 3302, 7450, 7679, 7380, 9955, 6087, 48, 4778, 1065, 3169, 3934, 4985, 9262, 4677, 2169, 9218, 3869, 7386, 2886, 1522, 2326, 7656, 8926, 6187, 549, 9457, 4294, 9283, 5297, 8548, 6448, 6482, 6825, 3220, 8826, 492, 870, 1705, 7878, 3977, 7774, 965, 7606, 3689, 1219, 5871, 2700, 889, 3352, 2337, 2654, 6679, 9069, 7127, 3964, 5893, 9715, 6106, 6100, 1793, 8127, 3068, 9020, 2213, 4285, 2105, 9665, 6735, 8846, 755, 2062, 3880, 1971, 8941, 1714, 9173, 9239, 7950, 456, 2327, 750, 3777, 7509, 9913, 5416, 5634, 8325, 7826, 7560, 7484, 5803, 8109, 5264, 5934, 1755, 1422, 9007, 5461, 1227, 3254, 8378, 1565, 2465, 281, 5288, 6688, 8057, 8618, 1921, 9131, 9907, 8480, 4296, 4680, 7923, 8551, 6347, 664, 1713, 7006, 3290, 4951, 9358, 1988, 8825, 5087, 8731, 7979, 1851, 2016, 7518, 9164, 7924, 1625, 929, 6562, 4348, 4764, 5983, 9764, 5908, 9928, 3223, 8679, 8374, 494, 7094, 7093, 7092, 7091, 7085, 7083, 7082, 7081, 6134, 6133, 6130, 6129, 6128, 6127, 6114, 6108, 6107, 6105, 6081, 6079, 6077, 6075, 6073, 6072, 6071, 6070, 6069, 6066, 6065, 6064, 6061, 6060, 6058, 6057, 6055, 6054, 6052, 3975, 3974, 5467, 3372, 7733, 7396, 6659, 9831, 7937, 8822, 8522, 854, 5038, 328, 7922, 5612, 5780, 3383, 8553, 4479, 6024, 9494, 5272, 1179, 1741, 1433, 609, 608, 9242, 6902, 9364, 8719, 6792, 4869, 2146, 8529, 7964, 5462, 1148, 1960, 8210, 8130, 823, 2259, 2248, 8848, 8332, 6555, 3339, 8221, 4051, 8432, 8319, 6337, 1641, 1842, 8156, 2365, 7860, 9517, 873, 2180, 2474, 2764, 2741, 799, 5676, 7765, 9148, 9695, 6313, 1029, 895, 6394, 3621, 7673, 4774, 3819, 3531, 4076, 7274, 4698, 5649, 6657, 8518, 3946, 4655, 6047, 1064, 6125, 6123, 6122, 6120, 6119, 6116, 6115, 8994, 6124, 6669, 2903, 4630, 2185, 2454, 8920, 3873, 4921, 9684, 6307, 9511, 5206, 9274, 6113, 3502, 2851, 6183, 5321, 899, 7330, 5280, 1045, 7993, 5845, 8550, 1918, 9421, 9371, 2456, 8170, 5730, 7879, 1908, 2850, 4502, 7090, 7075, 9154, 4456, 9527, 7394, 4492, 920, 7070, 9026, 5569, 8136, 642, 5095, 6680, 1668, 5245, 1039, 3267, 7154, 4866, 4286, 5563, 2644, 6651, 3563, 8696, 7906, 3256, 2196, 5344, 7353, 3490, 1820, 2992, 8906, 5002, 9123, 6025, 5447, 6662, 4048, 9309, 6253, 1185, 5078, 3066, 9146, 4753, 923, 893, 4686, 2103, 3718, 8166, 9043, 9892, 7911, 8734, 702, 3568, 9280, 1859, 3787, 7533, 8429, 9963, 4510, 2936, 2083, 7084, 7336, 4936, 2829, 8098, 9145, 1800, 3165, 214, 3020, 7628, 4108, 9672, 9679, 7519, 2469, 2294, 6225, 733, 4660, 7284, 2880, 2071, 1435, 4894, 94, 4910, 9476, 974, 3739, 5782, 2877, 8007, 7637, 3426, 9099, 717, 1448, 2812, 1771, 5249, 4107, 4311, 9788, 9984, 8640, 8080, 2281, 3519, 2432, 2991, 8138, 6513, 9056, 6561, 165, 818, 2782, 1429, 3461, 4578, 7088, 4797, 3845, 6288, 1089, 6434, 4362, 5839, 2562, 5181, 4292, 6564, 9596, 9922, 7223, 2661, 167, 8396, 5906, 2364, 7779, 7643, 3305, 6587, 9626, 2231, 1313, 5148, 3630, 2007, 9229, 1201, 3460, 3224, 3551, 7181, 2703, 8506, 9084, 3377, 8233, 5897, 3521, 6088, 5116, 3832, 5844, 8598, 192, 5880, 6891, 6716, 4527, 440, 8459, 8458, 9023, 7162, 644, 6559, 905, 3042, 1742, 5168, 5652, 249, 81, 8973, 5011, 8379, 6063, 9528, 3584, 4043, 5525, 5073, 2511, 8818, 5633, 1202, 4002, 9341, 6293, 1642, 8737, 9493, 3763, 9403, 8433, 8968, 8772, 7721, 4235, 813, 3447, 8012, 3821, 4464, 5399, 1566, 1993, 57, 9522, 8182, 5570, 4003, 6046, 9899, 6412, 3623, 8728, 8220, 7490, 2124, 8484, 2070, 8583, 4407, 5030, 5228, 7710, 3037, 3622, 4955, 8930, 726, 6691, 5354, 8198, 832, 6983, 3232, 461, 7814, 2664, 9411, 5618, 7259, 3736, 7980, 5819, 6430, 3231, 8264, 9405, 9002, 7279, 3582, 4646, 1218, 279, 274, 9354, 4979, 7965, 2643, 9533, 3572, 7460, 8058, 7881, 3136, 9424, 8934, 8827, 5083, 4380, 2688, 8896, 2211, 1125, 1966, 1023, 5519, 2134, 7713, 9923, 9458, 4881, 9339, 7685, 1198, 4860, 1408, 4832, 2458, 8392, 2539, 8528, 28, 7261, 928, 7425, 5384, 7256, 7592, 5428, 719, 5163, 7645, 5639, 783, 2881, 7552, 822, 6185, 8375, 2362, 8809, 6930, 480, 670, 3295, 287, 3722, 2375, 2156, 5801, 2860, 5650, 6094, 6463, 3303, 7570, 3007, 9611, 9080, 3687, 7433, 5598, 8688, 172, 7900, 7019, 2315, 9160, 2596, 3772, 716, 7215, 6596, 8723, 6779, 8, 1530, 617, 3416, 2329, 8390, 9910, 9660, 8093, 5796, 1238, 7395, 3534, 1950, 7325, 2820, 9246, 1516, 6449, 1424, 3833, 663, 4999, 1476, 9034, 684, 6580, 6469, 1132, 1611, 1890, 4947, 8478, 8650, 3227, 7212, 9536, 5744, 4993, 6237, 3166, 1127, 4708, 9770, 3455, 8020, 5504, 3991, 4658, 8571, 3228, 8398, 276, 1699, 1648, 7658, 968, 5397, 7139, 3351, 454, 5246, 3135, 3134, 478, 4336, 2157, 3176, 2095, 707, 7190, 95, 8519, 30, 5381, 9554, 8463, 2298, 7165, 9147, 4525, 6966, 6678, 8909, 3973, 8707, 3520, 9886, 3473, 5117, 9197, 2153, 8987, 2214, 9560, 4231, 3209, 7258, 9787, 1694, 4214, 2665, 9208, 5081, 5118, 1416, 8948, 2025, 2971, 9974, 9106, 4933, 9678, 1826, 4110, 8820, 9000, 1421, 9186, 1343, 8444, 3588, 4862, 3314, 208, 3674, 269, 2466, 638, 4971, 4929, 4928, 4569, 7669, 5426, 8775, 7755, 389, 3206, 2816, 9741, 5197, 5194, 2882, 924, 2372, 278, 1124, 4197, 6209, 6137, 9478, 4627, 5702, 9757, 880, 1337, 7994, 128, 921, 8542, 6494, 6504, 1087, 8496, 9805, 5644, 3543, 9269, 7095, 8991, 3202, 2307, 7214, 8657, 4782, 7461, 2100, 6731, 5622, 2356, 7304, 6308, 7145, 7144, 7598, 2253, 3424, 7389, 5509, 2246, 2348, 1569, 8422, 1609, 6348, 3073, 6621, 9738, 8718, 6839, 7458, 8501, 796, 5984, 4018, 5260, 5439, 6111, 6639, 2374, 1863, 9485, 2447, 5014, 8246, 2286, 4424, 4798, 4400, 4426, 4425, 3191, 6560, 6368, 6468, 6932, 8965, 3128, 8232, 3347, 6627, 1002, 3966, 8424, 7383, 7940, 9432, 5134, 8847, 7471, 4437, 1922, 2108, 7224, 6912, 1046, 1100, 6453, 5539, 4320, 3984, 8954, 5303, 5573, 37, 2271, 3723, 8471, 9369, 4145, 7173, 2370, 6692, 5195, 5491, 5342, 8800, 73, 2591, 9759, 5606, 9454, 1209, 7959, 557, 5912, 2150, 6611, 6767, 6766, 6761, 1205, 5522, 953, 2014, 143, 4006, 123, 4431, 6245, 3400, 2351, 4079, 8352, 338, 6840, 2675, 8874, 2667, 1587, 7237, 8924, 5589, 5145, 293, 8174, 3342, 7264, 791, 7861, 1665, 8076, 9782, 2411, 4716, 7953, 1841, 7722, 7939, 3896, 3957, 7571, 1207, 5376, 2479, 2099, 1577, 6650, 8755, 2681, 9773, 7491, 1097, 3130, 29, 7142, 8452, 711, 8504, 4283, 8190, 3782, 8410, 8641, 8530, 9362, 8321, 8143, 3311, 6093, 3956, 242, 4591, 3735, 4534, 8596, 9385, 141, 2673, 6799, 5882, 4396, 1110, 7754, 7663, 7997, 3012, 8441, 805, 8711, 5092, 5985, 1399, 3412, 9867, 9897, 7097, 9473, 7652, 3045, 8241, 7871, 1599, 2433, 5176, 245, 9095, 4075, 8189, 9437, 1899, 215, 4775, 9140, 277, 9916, 4391, 1779, 3610, 8079, 2018, 2399, 7574, 4219, 9024, 8202, 5910, 736, 2507, 7052, 5745, 1496, 4803, 4282, 9409, 6445, 4640, 5903, 423, 7718, 5610, 2545, 3596, 5593, 4299, 176, 9276, 7947, 2685, 4281, 8559, 5380, 6629, 1190, 6943, 9519, 7286, 6431, 4144, 979, 327, 5152, 1601, 173, 4387, 5093, 4790, 7891, 8205, 3749, 3780, 9414, 4854, 1022, 4697, 5748, 1300, 4683, 3326, 7761, 8985, 4089, 6977, 3289, 1537, 3509, 877, 1808, 6243, 1681, 5709, 612, 9506, 5442, 1043, 1481, 1457, 5345, 9894, 5060, 3327, 3745, 705, 5743, 1623, 6802, 2090, 147, 8310, 8451, 5527, 3910, 71, 5537, 1314, 9932, 2369, 5133, 8053, 7985, 2729, 9170, 6685, 1464, 3477, 9061, 4726, 604, 2353, 7463, 3879, 6797, 5198, 7436, 7776, 8147, 5646, 4871, 2452, 5561, 8898, 3430, 6247, 4779, 5242, 4280, 1717, 8249, 3283, 3609, 3605, 5388, 6218, 7661, 1471, 6254, 5252, 4886, 1895, 6401, 154, 2019, 8004, 5247, 9250, 8956, 7470, 4799, 270, 2512, 505, 8508, 2321, 2244, 4102, 8183, 1849, 6646, 6847, 9320, 8632, 2869, 4609, 2020, 6546, 9502, 6465, 9772, 221, 3336, 6074, 1928, 5427, 8585, 6174, 6173, 2707, 2484, 8590, 9855, 4270, 9317, 3495, 1956, 6843, 9906, 9543, 2376, 8014, 3071, 6205, 6204, 6202, 6201, 6200, 6199, 6196, 6193, 6192, 6191, 6189, 6186, 6180, 6179, 6178, 6177, 6172, 6171, 6167, 6166, 6163, 6161, 6155, 6197, 6195, 6188, 6175, 6170, 6169, 6168, 6162, 6158, 6157, 9215, 211, 2405, 216, 3343, 1631, 8041, 2678, 1534, 8975, 3428, 9950, 3860, 7708, 2787, 5665, 4965, 1994, 4813, 1570, 6553, 8983, 1555, 5487, 7901, 5335, 3530, 7045, 5662, 1618, 1620, 8353, 9064, 5524, 5139, 5856, 7723, 1407, 8397, 8794, 735, 8230, 9510, 2382, 9137, 3968, 4547, 7217, 6181, 6160, 304, 3691, 8627, 6566, 1782, 5837, 7829, 3067, 2691, 4583, 8591, 8229, 3242, 9501, 3617, 5119, 4139, 8071, 6203, 6973, 4517, 5000, 4589, 236, 5625, 7222, 7988, 9862, 9595, 6335, 4275, 1735, 7594, 820, 9188, 9786, 1593, 2542, 6784, 1617, 2663, 2052, 6974, 5507, 244, 7555, 2207, 5987, 1744, 8357, 8373, 7896, 6638, 4073, 6359, 35, 6512, 1795, 6424, 7898, 5365, 2296, 9442, 7734, 6393, 9869, 5003, 4529, 3238, 4316, 7528, 9323, 6976, 4361, 7998, 1347, 4515, 6104, 7745, 2188, 6785, 9302, 1858, 7209, 9905, 4914, 5514, 5041, 8971, 263, 7100, 8462, 6136, 5296, 5889, 4940, 1472, 7845, 2122, 4059, 3250, 9959, 947, 1379, 1004, 3198, 8656, 4037, 1785, 5781, 4090, 2226, 8766, 4436, 392, 4138, 3481, 8047, 6556, 7904, 3472, 7582, 5437, 9520, 9516, 5995, 9508, 275, 7244, 8725, 5566, 7820, 8925, 6219, 9340, 6489, 9136, 1037, 1075, 496, 5294, 6441, 1784, 8078, 5352, 908, 4931, 3485, 373, 284, 7416, 3836, 8383, 6433, 6594, 7293, 4760, 6270, 6406, 3158, 7501, 2509, 6008, 3803, 2671, 2822, 2720, 271, 8077, 8003, 5043, 2642, 9704, 2689, 3552, 4007, 5701, 626, 6391, 2319, 9044, 7517, 7828, 8234, 3431, 4730, 9720, 9291, 9060, 6013, 8005, 3310, 3613, 3734, 9943, 3244, 6096, 2457, 2859, 9944, 7180, 934, 2451, 2216, 4939, 228, 7857, 3982, 6451, 303, 6383, 3945, 5340, 5379, 3995, 9430, 8584, 3820, 3040, 4550, 8152, 3694, 8995, 5056, 6602, 9465, 7319, 682, 9754, 5726, 9419, 8167, 4416, 8438, 3661, 8953, 8204, 8380, 8278, 402, 6327, 1610, 9666, 977, 8026, 7554, 9933, 681, 6701, 8247, 8460, 3852, 1821, 2592, 6092, 2380, 8279, 8154, 4624, 7281, 2352, 5111, 1118, 902, 6972, 6894, 6311, 7547, 7544, 6255, 576, 2763, 2316, 2895, 9393, 4699, 3124, 5211, 4528, 2080, 1093, 4466, 6985, 4475, 8257, 1536, 2128, 2567, 3080, 9329, 887, 7542, 8884, 5746, 3838, 3585, 7766, 6686, 5751, 879, 786, 4970, 241, 9997, 5827, 7037, 14, 4542, 3341, 4128, 984, 1257, 6668, 1049, 5178, 748, 6486, 4858, 5209, 3719, 4679, 2726, 2092, 7686, 9748, 4905, 7812, 1276, 2324, 4315, 6118, 8566, 2289, 2288, 3239, 6726, 9420, 855, 7205, 5479, 7567, 268, 1317, 4681, 8998, 148, 9464, 2721, 1568, 6544, 1743, 7868, 8639, 3863, 7538, 9749, 9461, 252, 3388, 9471, 3028, 6496, 7846, 2769, 3783, 9440, 5601, 7292, 3562, 1684, 4192, 4373, 2330, 1647, 2152, 9484, 1868, 3729, 3394, 3810, 6006, 4541, 3971, 7374, 9322, 3744, 135, 5552, 9958, 5481, 5637, 2928, 1115, 5713, 5453, 416, 4576, 1904, 5826, 9497, 9803, 7247, 6717, 3614, 1126, 6117, 1661, 4631, 6729, 8937, 2223, 2106, 7379, 6617, 4339, 5054, 7684, 9179, 7294, 6882, 5240, 809, 2191, 7159, 8866, 2499, 1284, 834, 8316, 8901, 1840, 5576, 5647, 3498, 6312, 8021, 9860, 5528, 3683, 5747, 3458, 3350, 2345, 7401, 5847, 2530, 8660, 3595, 9035, 7561, 6528, 1686, 6317, 5596, 394, 8196, 7164, 7616, 3949, 2161, 5039, 2450, 8060, 9574, 9030, 101, 8206, 7513, 3823, 8833, 2261, 4765, 7483, 7014, 5692, 9105, 2045, 7504, 5867, 646, 5614, 9807, 8000, 8389, 2431, 2031, 1718, 5536, 265, 262, 7248, 7730, 3096, 9334, 3767, 3669, 113, 2817, 8075, 1903, 4678, 2241, 9872, 133, 7838, 4069, 9683, 2775, 2960, 2002, 6414, 653, 8168, 8609, 2935, 5392, 9380, 8549, 9468, 533, 3156, 1653, 6373, 1345, 7978, 8108, 1624, 6399, 1698, 3912, 3586, 3184, 6215, 3750, 5336, 4741, 3467, 7952, 9578, 2867, 3882, 2934, 706, 2937, 9355, 6531, 4863, 6156, 4693, 2541, 200, 7138, 4948, 455, 8473, 1067, 6913, 935, 2187, 3815, 2005, 5792, 9103, 623, 3178, 7287, 8103, 5776, 3667, 3051, 1881, 4410, 9551, 4057, 6595, 1090, 5732, 1220, 8647, 4291, 6830, 5372, 7903, 7150, 3612, 2112, 9569, 4471, 7367, 7060, 3790, 9237, 5818, 6567, 3188, 5179, 5401, 5220, 5556, 3816, 1941, 2193, 3565, 5219, 5052, 5928, 7821, 5090, 6790, 6613, 7366, 8615, 2400, 9976, 5298, 6768, 4300, 3139, 5423, 6711, 3676, 7729, 1353, 5913, 7699, 7596, 9224, 5351, 5494, 8837, 8188, 1836, 9793, 9521, 1727, 3948, 3492, 5886, 539, 1892, 3318, 260, 1459, 235, 4033, 7169, 493, 7816, 9085, 6294, 4597, 7195, 2401, 8467, 6315, 4329, 468, 4330, 8105, 8037, 4895, 9321, 3891, 5451, 2533, 2120, 5658, 4382, 800, 6770, 7469, 7307, 6782, 888, 8521, 6893, 8754, 3229, 38, 5474, 247, 4014, 8749, 8323, 4960, 8883, 6663, 4488, 1722, 6889, 9700, 4126, 4531, 1265, 8239, 3408, 7422, 568, 3812, 4277, 9762, 6456, 3122, 737, 7207, 6523, 9307, 6143, 4662, 6787, 5029, 1439, 431, 7035, 3848, 7178, 8437, 7036, 9074, 9107, 5157, 4650, 8532, 9914, 833, 8653, 63, 7431, 3246, 7865, 8708, 8541, 7403, 6464, 8726, 6109, 844, 7897, 5775, 149, 2670, 5348, 4963, 7434, 6011, 8343, 6673, 9285, 8262, 3094, 2676, 1500, 2130, 6398, 8773, 3756, 9014, 5918, 9412, 7192, 5244, 3870, 3512, 5443, 7216, 9306, 8201, 4133, 5164, 8284, 7807, 7238, 1054, 4327, 703, 288, 3685, 9168, 9558, 5929, 5275, 4777, 6048, 4893, 4440, 1696, 6781, 5631, 7666, 3478, 7657, 841, 8345, 7246, 9181, 8442, 9264, 2328, 647, 6257, 5322, 1180, 5822, 2250, 4385, 9973, 3199, 9052, 3221, 6103, 3574, 2778, 90, 8669, 7349, 470, 7099, 7473, 6365, 6831, 9801, 2892, 2618, 5306, 3233, 7, 826, 4164, 3692, 8340, 1695, 8919, 1396, 4620, 3133, 6641, 4054, 5400, 9116, 6756, 6747, 6135, 4842, 6147, 2514, 2843, 3841, 9365, 5774, 4897, 6509, 2826, 7887, 5670, 2733, 7267, 8479, 4318, 5063, 4621, 8334, 4290, 563, 582, 9572, 3894, 5170, 3364, 4194, 6240, 7634, 3576, 3313, 5378, 7668, 3942, 1949, 6880, 4695, 6557, 9745, 6855, 3702, 9977, 1561, 8738, 7612, 1789, 7111, 6356, 5690, 1864, 3765, 1662, 5901, 7771, 9979, 4582, 8687, 1028, 9433, 7532, 5229, 7853, 6507, 3409, 2774, 8876, 6479, 2684, 1634, 239, 9539, 9157, 6834, 5418, 4780, 168, 8786, 7062, 2832, 5640, 8875, 3701, 5186, 6036, 7301, 3014, 4445, 8449, 194, 4269, 2097, 7841, 8675, 3006, 4638, 8348, 3980, 1232, 7942, 5208, 5431, 5902, 9556, 1746, 9535, 5185, 2780, 2738, 4106, 2705, 5267, 4825, 6080, 6604, 3569, 7252, 4370, 3564, 9102, 7647, 759, 8879, 8877, 3567, 4237, 3360, 8619, 8836, 6426, 4227, 7908, 3893, 636, 1854, 2396, 8059, 5677, 1774, 6610, 3839, 2230, 114, 4794, 8384, 8268, 3385, 8453, 8881, 1691, 9396, 6332, 8163, 5707, 8102, 4876, 8982, 9661, 5436, 1318, 4017, 6522, 7487, 3996, 9109, 9251, 6838, 4312, 4301, 3673, 5334, 6, 2821, 8369, 4648, 9169, 1812, 5017, 67, 9391, 2347, 1669, 319, 2672, 6402, 9446, 5218, 522, 6969, 4061, 8838, 5613, 9068, 9978, 6620, 5169, 4563, 9565, 3988, 8052, 116, 8172, 6697, 9563, 9887, 4912, 1773, 3329, 9299, 9132, 9055, 1444, 1720, 5820, 1186, 2845, 2844, 4926, 4925, 4468, 3544, 9710, 9015, 4982, 8554, 7363, 4807, 7562, 545, 5585, 6900, 8145, 3425, 7681, 9374, 8729, 2783, 6703, 6819, 9673, 8545, 8386, 7352, 8165, 1567, 3606, 9402, 8251, 4792, 1703, 3540, 529, 8801, 2272, 5055, 5997, 4868, 6867, 2940, 5154, 335, 2841, 9918, 4486, 8850, 5225, 4163, 8562, 2669, 6801, 2217, 1169, 5033, 226, 5008, 2830, 1538, 1352, 4802, 7626, 995, 7876, 1139, 2608, 9189, 7526, 912, 6828, 7370, 8691, 9353, 6757, 751, 8469, 535, 5477, 8745, 4384, 6873, 6291, 4072, 1502, 4011, 9937, 5752, 6289, 1038, 4740, 7578, 3089, 2426, 9875, 3516, 7705, 1962, 2889, 5347, 9295, 5974, 27, 5798, 136, 795, 4487, 2550, 1395, 2750, 6495, 1564, 405, 606, 9452, 3353, 8370, 9363, 6599, 3592, 8468, 9777, 3591, 8770, 467, 2598, 3549, 7305, 5611, 7229, 8804, 8212, 3126, 7020, 6794, 7415, 779, 7644, 3909, 7913, 2827, 3001, 5557, 3864, 3261, 2013, 1622, 3700, 4501, 517, 6895, 4512, 4756, 2003, 8505, 2442, 6301, 2508, 6223, 5964, 8892, 6477, 9010, 9708, 7459, 9929, 926, 3000, 7588, 8281, 7676, 7890, 3362, 4959, 26, 3515, 2758, 2420, 5215, 7263, 6083, 1809, 4371, 6076, 3915, 3558, 7687, 9985, 2145, 2446, 2141, 9874, 150, 9053, 5324, 1096, 8367, 2687, 5577, 306, 6793, 7128, 5973, 1175, 2110, 2085, 2027, 2919, 8286, 9407, 8055, 4366, 6960, 6012, 1326, 8263, 3464, 9671, 7411, 6612, 9135, 4423, 6585, 1738, 3027, 9960, 6588, 6101, 3214, 8576, 9694, 1830, 4498, 6053, 6220, 8265, 8717, 6032, 7698, 6216, 9537, 3265, 2360, 6131, 2251, 9346, 6413, 6381, 6959, 4816, 8176, 9092, 4570, 3575, 789, 3633, 8186, 6806, 4596, 427, 9313, 6970, 1311, 4828, 9794, 3644, 7741, 1549, 4864, 6953, 1226, 8690, 2168, 6590, 2585, 9178, 1527, 3939, 5363, 9864, 4115, 5754, 757, 9200, 3902, 4801, 6338, 2381, 8194, 107, 5069, 6899, 4617, 6971, 2925, 7268, 1018, 7688, 4564, 9327, 290, 7888, 5542, 3525, 7232, 723, 6975, 6487, 8730, 4917, 2189, 9525, 1212, 4956, 6815, 7218, 4878, 5762, 4622, 1731, 2939, 2735, 4949, 8240, 4539, 1402, 5990, 8472, 3889, 2449, 3766, 7227, 4444, 3337, 8709, 3172, 9352, 3800, 8697, 5415, 9760, 6993, 6231, 7220, 743, 7995, 4747, 5072, 3642, 978, 1333, 3189, 9110, 3147, 9257, 3529, 6037, 2236, 7743, 9184, 3392, 6796, 8290, 5976, 4786, 6334, 2398, 2849, 8720, 6788, 7899, 3271, 6497, 2269, 6760, 8931, 8815, 9466, 4284, 7243, 9735, 8009, 9415, 7245, 7115, 4055, 9249, 2444, 3619, 8964, 6954, 7398, 5419, 2999, 4276, 1689, 7061, 2471, 1740, 6372, 2531, 3649, 6396, 8689, 9740, 2309, 5286, 2814, 2581, 988, 134, 8672, 7874, 3901, 1891, 473, 4343, 2573, 3696, 4540, 3598, 5349, 9449, 403, 7356, 4148, 6951, 6950, 3903, 9177, 9232, 5281, 4325, 9753, 7125, 7615, 3962, 386, 9498, 6420, 6817, 6600, 3396, 5289, 1200, 5389, 5180, 601, 2734, 4044, 3065, 5564, 2523, 4132, 5843, 5554, 280, 5971, 9870, 9500, 140, 9121, 8464, 7106, 704, 7421, 9982, 5568, 7951, 9891, 3593, 5963, 7894, 6484, 6743, 273, 1141, 1365, 1877, 8698, 657, 4394, 6995, 7047, 2961, 937, 7633, 5535, 8487, 6752, 8686, 8958, 7136, 8312, 4769, 4064, 6698, 8845, 6897, 7625, 5584, 2199, 3969, 9877, 6917, 9450, 1269, 4642, 2899, 3421, 6395, 1380, 2453, 3997, 2332, 6896, 6371, 3340, 6648, 3017, 5251, 8235, 696, 2094, 546, 8430, 2743, 794, 8684, 5430, 8916, 8228, 1959, 3710, 1113, 122, 6856, 2723, 1009, 1787, 9456, 4736, 770, 3222, 957, 69, 7317, 7862, 5870, 231, 1312, 6876, 5161, 3583, 4840, 8407, 8211, 79, 171, 815, 6885, 2587, 2824, 6488, 5446, 1463, 8256, 6068, 8180, 6798, 3577, 7759, 9231, 5265, 3338, 837, 8297, 8401, 6455, 7121, 4884, 3737, 109, 1732, 6578, 4405, 7587, 257, 4323, 8423, 7306, 5066, 6324, 3506, 4975, 7391, 4251, 610, 3801, 2486, 7365, 4173, 1628, 1, 8195, 6252, 2686, 4566, 8084, 2354, 6427, 8556, 4742, 3507, 699, 5828, 2012, 1920, 4288, 6732, 4643, 2797, 4927, 1092, 8132, 6095, 9693, 6786, 9721, 2151, 3680, 6233, 2278, 2846, 5572, 989, 7648, 9571, 4308, 6575, 7270, 6206, 5941, 3308, 4154, 6362, 4666, 4175, 6601, 2164, 2785, 9243, 7680, 9601, 7885, 7146, 62, 9240, 2740, 709, 1656, 7318, 8123, 2619, 4521, 8015, 534, 2727, 9347, 9946, 4034, 3269, 4500, 2793, 7008, 3183, 3578, 2441, 3380, 9400, 5143, 7982, 3297, 4562, 5325, 9077, 8769, 4120, 812, 8272, 7276, 5480, 4253, 2662, 6982, 9514, 2757, 6905, 7295, 2074, 2756, 7811, 7427, 4654, 169, 9941, 3010, 5489, 6418, 4193, 2593, 6583, 1605, 4180, 3732, 9972, 3253, 9040, 4093, 9314, 3539, 2367, 596, 7591, 8140, 8259, 9008, 4920, 9301, 7071, 3194, 5693, 4735, 9017, 876, 4623, 7364, 8699, 3376, 8339, 5883, 8280, 3018, 3162, 3368, 7409, 3960, 9090, 8526, 550, 4233, 1498, 3713, 5765, 2008, 5422, 5786, 4267, 1672, 3078, 1480, 6640, 2544, 166, 2718, 9876, 8580, 6410, 6472, 8620, 1861, 7210, 4174, 2761, 593, 7659, 1244, 3854, 3900, 6739, 5331, 5834, 3627, 3853, 9677, 3890, 5609, 6681, 233, 8393, 3193, 8710, 7296, 4682, 4083, 5445, 1749, 6904, 4232, 6506, 8072, 5878, 4474, 2828, 7711, 2060, 7914, 7883, 2911, 6853, 9187, 441, 1866, 710, 9857, 6467, 4986, 6392, 9098, 4381, 4032, 9429, 1456, 8197, 7041, 3324, 9118, 9909, 7649, 5234, 8330, 6965, 5214, 7917, 2921, 1970, 2569, 2913, 4328, 975, 258, 9871, 7324, 2228, 6150, 177, 420, 3004, 8101, 5547, 2084, 400, 7340, 9729, 1050, 7462, 3580, 370, 8989, 9372, 605, 7297, 5444, 2831, 5346, 3164, 7768, 9475, 3672, 5226, 5750, 6632, 4554, 9155, 2349, 7559, 2343, 1497, 5688, 2715, 4669, 4244, 9784, 3594, 4252, 8613, 4701, 634, 7763, 266, 1554, 7992, 5469, 6628, 4484, 7833, 8715, 7221, 5607, 8461, 9435, 9534, 2553, 6667, 1996, 8612, 5529, 7044, 2029, 9398, 9097, 7155, 5012, 3875, 916, 8500, 551, 1598, 6212, 489, 9108, 1862, 6466, 8864, 1637, 619, 8250, 2435, 9279, 801, 15, 4118, 9233, 4751, 5490, 8160, 9686, 2628, 5236, 1796, 1660, 9389, 9765, 7505, 9202, 2728, 9359, 6039, 8016, 5763, 7696, 8945, 3309, 932, 4754, 4170, 4579, 6700, 9025, 2339, 2149, 8567, 9680, 560, 7126, 4599, 7568, 3597, 5486, 6634, 310, 3181, 4538, 6890, 1501, 8940, 152, 1535, 8104, 5458, 8597, 5587, 1777, 5759, 5358, 5875, 1688, 6535, 6699, 5472, 42, 7639, 1072, 8276, 415, 7584, 8810, 718, 9462, 3307, 7682, 3420, 3264, 830, 4598, 4372, 2702, 9149, 9332, 9005, 7219, 9190, 4930, 3618, 7921, 6388, 9191, 9711, 2912, 7847, 5432, 5711, 867, 5590, 9942, 3423, 7577, 7714, 9241, 6316, 5898, 1832, 8918, 188, 8114, 8821, 9975, 8676, 2621, 1253, 2776, 210, 9351, 5032, 8333, 7646, 882, 731, 9936, 1140, 2888, 5588, 6461, 9957, 3814, 5965, 7329, 6763, 4857, 4414, 1249, 3599, 910, 4458, 8606, 2079, 5558, 4207, 9124, 8897, 9112, 1757, 7557, 8381, 202, 9300, 5698, 9129, 191, 8349, 9557, 5402, 6450, 864, 8561, 4091, 3797, 8260, 206, 2566, 1484, 1161, 2448, 4520, 3333, 8067, 3697, 103, 5841, 7096, 8891, 7323, 9698, 5860, 8038, 2359, 1531, 7265, 7955, 3365, 1397, 1136, 3802, 5515, 5881, 4526, 9057, 5383, 9756, 7167, 3532, 4135, 7362, 3140, 9114, 2222, 2639, 1750, 16, 2746, 3235, 3448, 853, 3373, 6403, 8213, 4734, 9438, 8017, 1969, 5567, 7576, 5905, 9370, 6239, 3904, 5227, 8254, 9078, 2313, 8350, 1083, 1875, 8993, 8943, 6622, 4505, 1020, 621, 6645, 3932, 6344, 7916, 6939, 8703, 2183, 2988, 8961, 2989, 1724, 1855, 9705, 8097, 7512, 1573, 2133, 2132, 2131, 2127, 2982, 9969, 2915, 9062, 469, 9477, 4592, 2759, 7404, 4626, 3847, 2275, 2154, 6474, 3679, 4096, 1473, 4435, 3262, 1946, 556, 5761, 9981, 1431, 784, 8448, 804, 8124, 994, 5885, 7695, 8849, 7291, 7489, 754, 5800, 6023, 6803, 9172, 1953, 2366, 3312, 8744, 8161, 8796, 2977, 5132, 8034, 7066, 3119, 5243, 2974, 2653, 7069, 6722, 9820, 6322, 3259, 9917, 9258, 8022, 8309, 6694, 1846, 5307, 4691, 5406, 5128, 7858, 3270, 6285, 1907, 7202, 9824, 3938, 2462, 1635, 4514, 9889, 9268, 6446, 2350, 9567, 7200, 5097, 9161, 5064, 2683, 2263, 9344, 6942, 5495, 5391, 884, 7057, 3587, 3695, 9948, 1032, 3917, 9568, 4636, 9356, 8287, 7912, 8372, 2490, 174, 3076, 4004, 7672, 6810, 7399, 8751, 1977, 5981, 7442, 32, 7987, 1252, 4041, 4675, 4256, 5619, 9995, 89, 3761, 3366, 6637, 2021, 6379, 5207, 7707, 2125, 9752, 7724, 1980, 9630, 9829, 9213, 7250, 2265, 8605, 2897, 9343, 907, 9472, 3375, 6926, 160, 4063, 7299, 1301, 2958, 4015, 2459, 2148, 9310, 2427, 9153, 8558, 3237, 9348, 4202, 6813, 4490, 5632, 9804, 6367, 5290, 199, 7012, 3317, 8808, 4809, 8112, 3989, 7859, 1576, 5846, 1101, 4968, 230, 1160, 2694, 7660, 6363, 2748, 1853, 7827, 7621, 3331, 1324, 2172, 1486, 597, 4279, 9980, 6655, 8129, 3434, 119, 4651, 5212, 7449, 7642, 4504, 3395, 6397, 7228, 9081, 4731, 4990, 359, 7855, 7825, 5810, 291, 2522, 1919, 985, 4305, 5465, 3369, 4297, 1458, 3171, 6084, 4980, 2358, 777, 3877, 6370, 8887, 2299, 9737, 5153, 5663, 8536, 8535, 7051, 3291, 4153, 5466, 4023, 254, 9727, 2914, 4977, 4957, 9428, 970, 7849, 8527, 7289, 1788, 8295, 4652, 3003, 6979, 8990, 1281, 7522, 4248, 5766, 8608, 41, 8791, 4442, 308, 9292, 6508, 5954, 538, 2952, 1816, 7530, 7003, 7002, 8869, 3684, 2865, 5360, 6909, 2040, 8611, 798, 5488, 5425, 3393, 4065, 6387, 2064, 6027, 7907, 1292, 1495, 163, 5617, 54, 860, 9730, 3346, 53, 1149, 7432, 9561, 5575, 8727, 1995, 3965, 9434, 1325, 4946, 2635, 1242, 9790, 1579, 7233, 5103, 8784, 5863, 1517, 7520, 289, 6754, 6165, 7840, 5315, 7048, 3924, 6907, 3807, 980, 6408, 3046, 4460, 5879, 4767, 9549, 2896, 4074, 9669, 5047, 5070, 5129, 2143, 93, 77, 5020, 8394, 5409, 2322, 2044, 9182, 3345, 8492, 1228, 7001, 8158, 3005, 839, 9731, 2057, 11, 9004, 8425, 9902, 9644, 1446, 8044, 9621, 9492, 8086, 4995, 8856, 3299, 7055, 1945, 5448, 9273, 7948, 3645, 2580, 6140, 3072, 2235, 5433, 4200, 573, 5269, 2220, 7895, 8486, 6001, 3023, 2077, 2109, 7480, 4634, 5785, 6941, 1044, 9947, 7581, 2617, 3725, 8337, 145, 8477, 829, 3098, 1366, 3986, 2487, 2978, 9504, 6296, 3895, 4725, 5804, 7957, 5284, 1523, 6336, 2959, 7282, 6458, 810, 1143, 8395, 1163, 6304, 3491, 7040, 7335, 5310, 2416, 7118, 9376, 4338, 3081, 4365, 7496, 861, 7272, 6996, 8025, 2046, 5884, 1528, 5629, 2641, 1174, 3398, 9223, 5210, 9481, 8215, 8872, 5736, 8391, 4109, 3662, 4873, 5414, 5034, 5031, 8568, 6318, 697, 5291, 5551, 418, 3876, 7782, 9308, 8293, 2055, 9101, 3784, 8685, 8531, 9367, 358, 4882, 2504, 3358, 6440, 1917, 2464, 7599, 901, 5464, 8344, 1295, 7116, 1042, 1177, 2377, 9261, 100, 9144, 45, 8763, 9562, 7775, 124, 6400, 9382, 7889, 8244, 1340, 3132, 8942, 7949, 8895, 9216, 1010, 8294, 1999, 982, 7430, 6922, 8716, 1867, 5276, 4548, 8328, 3441, 7655, 8835, 8661, 4349, 8638, 9732, 9670, 3647, 1887, 2953, 1526, 729, 7351, 3296, 1398, 2658, 6558, 4715, 4714, 4105, 4350, 9849, 1645, 6745, 5795, 7072, 4241, 9919, 4046, 3693, 6952, 8873, 9094, 9139, 9009, 1068, 9796, 5492, 6152, 4637, 7529, 8938, 4732, 9159, 5695, 1690, 8056, 8700, 9524, 1241, 3384, 1056, 4892, 4243, 4357, 6236, 482, 5874, 6211, 1655, 6574, 4635, 8326, 2625, 1104, 5996, 6471, 2091, 3211, 8131, 3272, 7063, 6945, 4907, 1153, 1247, 6385, 7322, 603, 843, 8266, 78, 8274, 5071, 8364, 5338, 1621, 2043, 1932, 6421, 520, 3518, 1239, 5927, 4849, 1414, 3706, 9723, 7556, 8193, 4954, 5048, 5562, 4887, 9183, 6999, 4649, 6987, 7331, 162, 3177, 1279, 1571, 8269, 3657, 8774, 7927, 6724, 5394, 3195, 435, 9399, 3615, 217, 6598, 6214, 3275, 3831, 2879, 5104, 86, 7412, 6514, 6772, 9728, 4749, 5204, 5574, 1551, 6944, 8778, 2209, 1590, 1520, 7358, 9734, 3504, 7201, 5356, 8090, 9180, 5972, 7443, 939, 5302, 4455, 5018, 7101, 7160, 8630, 6303, 7709, 3145, 6232, 3454, 1030, 7664, 10, 3941, 5044, 802, 8362, 3760, 8314, 7239, 6539, 2413, 5327, 2344, 1857, 7447, 2885, 2884, 7039, 6994, 6146, 1916, 6390, 519, 7029, 7752, 5266, 5694, 6360, 2980, 7624, 9823, 4462, 2795, 4351, 7321, 2287, 5339, 3634, 2200, 4476, 7739, 3205, 2521, 6660, 1499, 4287, 5840, 9455, 178, 7130, 6490, 7347, 2011, 7373, 8243, 7589, 5420, 4422, 311, 7042, 7809, 1678, 6837, 8714, 7977, 7976, 4633, 7946, 1578, 9900, 3635, 6298, 3827, 1319, 656, 5149, 157, 6272, 5814, 5473, 3474, 7191, 1644, 8654, 4045, 2875, 5689, 2177, 6967, 1122, 6573, 9676, 9338, 7277, 5450, 7958, 1733, 5706, 654, 9771, 137, 8523, 4495, 5478, 9873, 1600, 8155, 7338, 9968, 7117, 2341, 4203, 9211, 906, 7107, 6300, 4353, 8267, 6366, 2190, 2680, 1164, 1947, 1445, 201, 6742, 2556, 1231, 463, 5136, 6208, 6789, 6696, 2089, 7926, 96, 6035, 5140, 578, 8515, 50, 6004, 5873, 7193, 8789, 9331, 504, 9128, 3804, 724, 9388, 8371, 7455, 3167, 2041, 8705, 4493, 3717, 6265, 4889, 3486, 1828, 5300, 8488, 2144, 5687, 6475, 8447, 2463, 2560, 631, 4967, 8491, 5172, 5171, 6234, 5636, 5417, 4335, 3212, 9469, 6297, 3293, 7426, 159, 3106, 3355, 4606, 7788, 8844, 6159, 6804, 4546, 4661, 4744, 6800, 3626, 393, 4317, 6823, 2614, 7627, 5718, 7166, 8936, 8490, 9111, 3024, 8988, 6842, 4019, 3733, 7819, 7465, 2256, 747, 9349, 9815, 6241, 8089, 6769, 8142, 9406, 5257, 6275, 4817, 874, 8482, 2422, 9075, 7355, 2026, 7716, 5978, 4010, 9893, 1633, 8537, 126, 6141, 677, 1730, 5405, 8762, 9487, 3294, 2811, 7242, 1596, 9330, 4485, 7199, 9059, 3560, 4593, 3187, 918, 7359, 5201, 4247, 1591, 2, 6805, 7933, 8917, 3438, 6271, 6759, 9628, 8589, 3453, 2815, 5141, 561, 5926, 2212, 6836, 8976, 4890, 6921, 297, 5106, 4943, 9927, 6923, 513, 5001, 6603, 1827, 8966, 8001, 8088, 7253, 9783, 6515, 8974, 256, 4844, 5086, 3276, 4171, 3397, 5714, 5316, 6901, 4557, 8662, 2197, 4427, 8652, 9480, 3252, 438, 803, 5312, 6852, 8179, 2355, 1080, 526, 4836, 189, 9576, 3319, 7257, 309, 3470, 3226, 9577, 8207, 484, 6452, 4266, 4012, 8739, 3344, 713, 7303, 4841, 8092, 598, 7593, 6435, 9368, 9585, 4629, 3391, 3093, 7999, 243, 3390, 7678, 3105, 146, 6529, 8886, 4236, 3508, 4412, 8722, 3648, 7511, 3601, 6636, 1488, 5560, 1798, 5967, 9714, 2868, 8252, 3201, 4035, 4602, 4600, 6442, 4230, 9104, 6814, 248, 9780, 5850, 3600, 1291, 502, 4720, 2813, 5271, 2009, 197, 1734, 6615, 1230, 8018, 6207, 6026, 8061, 5396, 9027, 5656, 3435, 4494, 8882, 9674, 7602, 6380, 8569, 7204, 8150, 4254, 6924, 2943, 4165, 7034, 375, 2264, 4969, 4668, 1701, 7892, 7184, 5107, 3103, 1882, 72, 5058, 472, 6962, 909, 816, 2126, 2210, 4992, 7495, 5323, 2485, 8049, 7196, 7712, 7260, 7141, 6483, 4169, 1147, 1233, 6121, 4687, 9047, 5506, 5729, 1997, 1142, 1630, 4265, 9324, 8083, 4021, 1086, 4808, 3792, 2996, 161, 2268, 7984, 3371, 3192, 9293, 863, 8890, 8270, 4953, 4935, 5697, 6997, 5084, 1350, 1929, 7822, 3785, 2049, 7618, 9199, 3846, 175, 7137, 1951, 9755, 7005, 2293, 6541, 205, 3992, 7783, 1266, 8802, 8695, 5299, 3659, 6139, 1767, 2501, 7835, 76, 890, 4434, 3728, 9063, 9115, 3759, 7481, 2725, 3057, 1359, 781, 3085, 6050, 5943, 3935, 6022, 9789, 3437, 1998, 5270, 8671, 1984, 7479, 5147, 5740, 9083, 462, 7320, 104, 4689, 7830, 4671, 3159, 1106, 3776, 3137, 7806, 7875, 7290, 2481, 9724, 6033, 1048, 5057, 3493, 3724, 6144, 2724, 9779, 1553, 301, 6277, 1369, 2255, 5098, 5375, 5175, 806, 9761, 4721, 3885, 4618, 8949, 6875, 5441, 5455, 2906, 6263, 2311, 7234, 3370, 4738, 5723, 651, 8126, 1957, 5861, 4067, 5991, 6563, 3727, 7203, 7781, 9813, 8575, 4185, 8493, 7406, 5452, 9162, 8122, 7328, 958, 6182, 7188, 5403, 8258, 264, 6888, 2536, 4604, 5727, 4988, 9743, 3475, 312, 9006, 9312, 396, 1626, 3417, 6278, 376, 6000, 13, 2956, 9272, 7211, 1865, 9733, 4613, 318, 5812, 1615, 8296, 8819, 4080, 9254, 5217, 22, 4473, 4221, 6326, 8865, 7632, 2429, 8587, 9453, 4143, 3501, 8539, 3824, 7928, 4397, 4347, 1310, 2179, 2310, 8678, 2489, 8209, 9703, 4991, 2973, 4784, 1723, 4250, 8516, 7983, 6246, 1792, 155, 7123, 7354, 1419, 7969, 8610, 4415, 9150, 3031, 7970, 3855, 9800, 2394, 118, 8602, 7689, 3047, 7794, 3039, 6229, 851, 782, 8100, 4078, 4206, 8336, 1409, 3753, 1888, 6710, 9763, 5778, 6190, 9970, 61, 2866, 125, 7342, 8771, 4718, 6568, 3114, 620, 8944, 6980, 9744, 5977, 5046, 3288, 2682, 2932, 9924, 4758, 4565, 2510, 1081, 6358, 6505, 7327, 4262, 2082, 9863, 4359, 6249, 6538, 5719, 2333, 5994, 1278, 4249, 4690, 3487, 4104, 4619, 3561, 9212, 2599, 6822, 4543, 4130, 7856, 8765, 2087, 9758, 5370, 6886, 5523, 2181, 3115, 8199, 840, 8547, 6695, 8674, 9266, 6570, 6569, 9736, 5326, 8921, 1123, 5449, 8693, 9523, 3348, 5603, 1403, 6250, 1926, 1008, 5190, 6145, 7635, 3681, 5235, 5787, 758, 5122, 1943, 5144, 2232, 2495, 4590, 5184, 4684, 9747, 683, 5855, 2482, 3688, 1144, 1454, 495, 4273, 2613, 3716, 5355, 5138, 7726, 2557, 190, 5387, 7508, 4113, 7636, 6384, 7989, 7441, 1334, 2242, 3180, 6357, 3758, 3381, 9042, 6665, 295, 5191, 7010, 6029, 1302, 5088, 7494, 8600, 8495, 5510, 3698, 7531, 2434, 516, 3175, 2221, 6126, 203, 3624, 5992, 9541, 299, 4575, 5283, 97, 7704, 721, 9467, 2233, 9992, 5200, 8305, 6938, 4806, 2997, 4628, 7059, 1243, 1725, 267, 6846, 3219, 5581, 5411, 8851, 7784, 1449, 1336, 6591, 2202, 2204, 2737, 5099, 2792, 1474, 2779, 6049, 4176, 9915, 7665, 4177, 9818, 1572, 2483, 4874, 2626, 8288, 5382, 6824, 7453, 9127, 6016, 9725, 6447, 7283, 3129, 9091, 3923, 7275, 398, 7348, 9384, 6386, 3009, 7369, 8324, 9423, 9769, 9940, 232, 8443, 7935, 7902, 4978, 5343, 9926, 2730, 7654, 5628, 1592, 7054, 3379, 2415, 7185, 9070, 9255, 6256, 8635, 8743, 3899, 8967, 3488, 5075, 1675, 8799, 3983, 6552, 7925, 4156, 827, 4696, 3469, 7735, 2612, 5262, 9304, 6775, 5476, 5710, 6919, 307, 7569, 6964, 4509, 4178, 4208, 5797, 5779, 2716, 4499, 2752, 9012, 5019, 3929, 6753, 9966, 3950, 5142, 5239, 8498, 3489, 443, 4450, 8439, 5094, 5484, 8724, 6530, 808, 6534, 9781, 5059, 9201, 9142, 4040, 1588, 9898, 5173, 2383, 9318, 4595, 8979, 1934, 60, 2713, 1560, 1559, 4356, 3363, 1293, 1563, 2863, 8445, 8125, 3084, 1178, 1102, 6582, 1728, 2056, 9287, 8214, 3791, 4768, 2649, 7206, 6963, 3985, 6934, 9408, 668, 8524, 7350, 8546, 4419, 4239, 6572, 9802, 936, 607, 6910, 7804, 627, 255, 6791, 4242, 6955, 2137, 1119, 9289, 4513, 1348, 4272, 4641, 6677, 2751, 7119, 555, 3321, 3806, 3660, 99, 9436, 7864, 6110, 5189, 9425, 1091, 8741, 3266, 9271, 1685, 5664, 8814, 3837, 1145, 9387, 1643, 807, 7929, 9195, 821, 3160, 5583, 3008, 6019, 5597, 2563, 9210, 1428, 9225, 6038, 4326, 1250, 8418, 5829, 3843, 5367, 1739, 6439, 7339, 313, 6015, 5005, 6807, 1364, 1663, 4586, 9076, 4518, 4114, 2650, 9791, 7551, 6883, 3494, 9079, 4211, 1215, 785, 9427, 7516, 5890, 3117, 2985, 3070, 9499, 8512, 6525, 3955, 639, 9883, 2030, 31, 4788, 6947, 941, 1775, 5304, 8217, 3829, 4298, 1103, 1961, 4950, 5602, 4614, 1658, 9196, 4877, 5238, 4413, 9709, 3086, 9685, 5659, 7143, 3897, 7466, 6906, 3840, 6405, 3954, 5783, 8592, 4209, 5314, 6577, 5053, 6542, 6411, 7527, 8173, 8588, 4783, 7565, 5788, 8628, 4915, 8574, 5177, 1817, 7506, 1011, 4704, 6248, 3865, 6764, 1225, 1206, 6579, 6551, 7545, 9584, 8756, 9858, 1195, 213, 1356, 8852, 6321, 5948, 4676, 9011, 8144, 5105, 6028, 4572, 4123, 7122, 7361, 5287, 1991, 2630, 2443, 4215, 6808, 7742, 7451, 8033, 6984, 8683, 4039, 7423, 2182, 8899, 5769, 1680, 6730, 9386, 4508, 5193, 4916, 8177, 8200, 6457, 7880, 1222, 51, 4162, 1296, 4511, 1603, 6991, 9390, 5968, 624, 8043, 8634, 9515, 8969, 3050, 1667, 3287, 3998, 5274, 2957, 4870, 2954, 1745, 8736, 3550, 2732, 6524, 4324, 9316, 2976, 6750, 6749, 6748, 5919, 9357, 7749, 4507, 9252, 3349, 6746, 2962, 362, 6725, 2833, 3868, 7566, 973, 9615, 9167, 2704, 4835, 9655, 6854, 666, 1082, 6654, 8408, 8409, 7738, 4234, 3325, 3822, 4389, 6908, 2292, 4829, 7934, 4587, 7815, 2916, 7605, 4369, 6184, 742, 3715, 8411, 3730, 824, 8970, 9335, 5772, 8645, 8681, 1884, 507, 1900, 7298, 3505, 339, 8069, 6940, 4465, 9861, 8139, 9328, 6741, 3330, 251, 5009, 5503, 4111, 6948, 3637, 6429, 9260, 4056, 4594, 1170, 6898, 4116, 2891, 3872, 4899, 88, 9315, 237, 1906, 5501, 7674, 4703, 3306, 7981, 5809, 7805, 3866, 1394, 212, 2576, 5638, 4420, 4880, 5858, 6581, 7692, 4923, 121, 426, 1315, 2410, 9631, 2497, 2492, 5051, 298, 6857, 9031, 4222, 858, 5016, 1374, 7179, 9512, 9119, 3655, 4257, 6286, 5222, 6417, 1811, 9204, 6290, 198, 3483, 2006, 7573, 6149, 7343, 6619, 4001, 9265, 5513, 8996, 565, 56, 156, 6262, 2525, 2317, 5916, 9667, 6377, 6480, 1683, 223, 5939, 9288, 1716, 8094, 9531, 5865, 5221, 8489, 3517, 6210, 661, 3527, 3083, 8617, 3443, 4337, 5793, 6589, 7938, 1066, 459, 4748, 4824, 2282, 2706, 6346, 4937, 7310, 2240, 5460, 1944, 1286, 694, 1461, 7086, 4050, 6493, 1131, 6485, 3328, 3918, 7341, 8893, 2520, 4522, 3032, 1321, 7089, 2710, 1024, 4344, 6194, 2873, 7719, 2117, 3773, 3652, 6715, 8455, 428, 7936, 4188, 4184, 6998, 6734, 4183, 6520, 4932, 5669, 85, 2864, 43, 4555, 9165, 764, 1320, 6933, 9548, 1487, 5362, 3111, 1351, 6478, 5683, 6536, 8483, 2835, 9263, 2234, 4603, 1469, 8578, 2323, 9956, 4417, 4022, 7764, 2518, 9573, 3704, 5045, 1297, 9198, 6709, 8581, 2331, 4081, 530, 6545, 5679, 6758, 6609, 5876, 9939, 490, 7475, 4199, 5907, 4293, 6548, 4377, 8238, 3878, 6771, 5737, 6674, 1151, 8999, 6151, 4077, 5961, 7747, 1389, 8412, 4103, 7194, 3456, 333, 9930, 117, 2067, 8191, 7919, 8601, 5421, 7608, 850, 4482, 4545, 4378, 1058, 5475, 8832, 129, 2494, 4605, 1575, 6224, 7013, 1819, 2402, 2404, 6314, 641, 6693, 8859, 7393, 1806, 3656, 7697, 3638, 9503, 6176, 8068, 9692, 9361, 4334, 5429, 7262, 8096, 8842, 1285, 8184, 6712, 1802, 1801, 19, 2634, 8224, 2745, 5851, 4584, 1224, 5849, 3190, 3608, 7049, 4483, 1108, 2325, 9013, 1453, 5620, 3640, 1016, 8783, 5085, 7333, 1650, 3607, 6652, 6259, 130, 8042, 3834, 3711, 3743, 7311, 7507, 3058, 518, 7251, 2674, 6276, 3611, 8299, 9713, 8382, 524, 25, 2575, 8534, 1455, 2201, 4159, 204, 797, 9033, 5096, 3402, 7619, 453, 5361, 9885, 673, 8420, 1815, 1021, 2503, 637, 4766, 7025, 5852, 3526, 138, 1911, 4497, 2178, 6592, 5232, 4981, 7080, 7269, 8955, 6608, 1013, 5424, 2804, 5559, 7446, 9688, 6533, 7387, 6510, 7360, 8713, 1452, 8031, 3217, 9206, 8564, 1797, 2794, 7535, 4393, 1664, 2167, 4644, 4639, 80, 322, 6549, 465, 4964, 7839, 5738, 2709, 4094, 5530, 7760, 7823, 4743, 774, 6736, 7474, 9404, 2245, 7650, 3522, 9859, 4506, 9495, 1196, 9882, 8776, 4388, 6416, 2088, 4544, 6274, 9719, 625, 7371, 1799, 7344, 8045, 6928, 8342, 3555, 7078, 8735, 272, 1287, 4737, 4810, 8764, 1084, 6537, 183, 182, 181, 180, 179, 1612, 6915, 3413, 4068, 4901, 9903, 3056, 3127, 3513, 8904, 3120, 1519, 1309, 5512, 9658, 9776, 8946, 5079, 9690, 5811, 5674, 5517, 5764, 3604, 2461, 1876, 8577, 8149, 4757, 4896, 3466, 6018, 2538, 6056, 7631, 3554, 8085, 9513, 7129, 7009, 3155, 397, 9540, 8748, 8099, 9439, 532, 2564, 4656, 3332, 3867, 9175, 5923, 343, 1657, 8563, 5076, 2038, 1339, 6310, 3179, 3888, 7124, 4822, 6266, 1430, 7910, 8208, 6500, 527, 5137, 3830, 9566, 6415, 8065, 4220, 1468, 5993, 8511, 9392, 4727, 7620, 3143, 1824, 7472, 6409, 3116, 1264, 3636, 5904, 8950, 4911, 1710, 2927, 6990, 508, 5413, 4210, 8308, 8811, 7785, 7787, 5888, 1272, 8341, 6521, 7769, 2760, 1885, 4031, 2516, 5285, 4224, 7848, 9989, 7031, 3110, 4260, 5940, 5301, 3060, 9837, 4759, 8759, 2781, 9994, 7786, 4615, 8790, 4062, 4830, 7873, 9505, 2699, 5586, 170, 5213, 8054, 7790, 356, 4259, 8908, 5553, 6644, 8817, 5604, 8607, 7731, 8436, 2836, 845, 24, 5667, 2037, 5508, 5742, 3468, 9284, 5150, 5935, 3282, 9945, 7803, 7700, 9697, 8540, 9911, 1874, 6148, 4205, 9822, 9143, 9088, 3500, 6832, 6723, 2848, 6097, 3200, 7558, 302, 4268, 8740, 4261, 650, 8621, 1193, 8222, 8219, 1443, 9746, 5114, 2257, 7187, 3300, 4851, 2505, 6702, 4213, 3570, 5254, 2440, 8435, 6626, 7156, 1850, 1055, 3382, 8758, 911, 4263, 5511, 2605, 8750, 2419, 1450, 3465, 9305, 227, 3499, 2273, 6269, 2640, 1829, 4066, 4454, 5831, 5065, 5295, 8434, 9360, 3874, 7728, 9022, 2537, 8752, 3639, 7837, 1331, 6378, 9701, 3281, 6642, 7454, 776, 8586, 9176, 1831, 5931, 7553, 8747, 5404, 4853, 3141, 8292, 6675, 9908, 6268, 2963, 1041, 7667, 8302, 7198, 2306, 4418, 4070, 1355, 1203, 9486, 5080, 8313, 5942, 1869, 1762, 6473, 4439, 485, 1813, 2528, 828, 2949, 4791, 2470, 9888, 447, 7793, 8066, 9120, 6714, 3026, 442, 4729, 1182, 6043, 6261, 6407, 763, 3142, 8782, 3994, 1606, 2917, 9219, 515, 775, 4461, 5682, 1420, 7112, 8780, 559, 2488, 5330, 4218, 4469, 771, 1094, 9082, 7408, 5716, 633, 1236, 3144, 4367, 9546, 479, 8303, 8231, 3279, 6705, 6625, 111, 7418, 9809, 2941, 2285, 9865, 1392, 2878, 2823, 1607, 127, 2114, 4900, 8797, 6034, 4020, 3545, 9620, 3670, 2615, 5541, 1585, 2631, 5624, 5937, 7023, 5131, 7027, 2874, 5438, 3082, 196, 5166, 1335, 8417, 6937, 1208, 8977, 3769, 6738, 3161, 2648, 8118, 2529, 1697, 6355, 618, 5966, 9113, 3482, 1679, 2384, 1981, 5109, 7417, 46, 7972, 740, 404, 7630, 7777, 8509, 5959, 9637, 8538, 848, 7407, 1521, 449, 8603, 7482, 6306, 491, 9214, 3768, 5955, 5565, 9629, 9839, 1088, 3038, 234, 3818, 4984, 4027, 5026, 8301, 9041, 8932, 6718, 9901, 4289, 1211, 8643, 3844, 5949, 3090, 2195, 3445, 4558, 9418, 5061, 8476, 1758, 6992, 353, 8128, 2028, 865, 4030, 7778, 2412, 1753, 7973, 3446, 9003, 2965, 21, 361, 9838, 6988, 3359, 6078, 9591, 5945, 5944, 9592, 7971, 4225, 5938, 1711, 5999, 5548, 7016, 9530, 8050, 1614, 8894, 7026, 5953, 8087, 8502, 9844, 766, 6733, 7015, 7991, 9529, 2773, 8573, 6376, 2408, 5951, 648, 9579, 2771, 7690, 5956, 3410, 1872, 5532, 6361, 9422, 2861, 2500, 7017, 4834, 8544, 1479, 5599, 9845, 7033, 9652, 9810, 3405, 1557, 2010, 6605, 4536, 9602, 1299, 1298, 8082, 5412, 5950, 7018, 5946, 3944, 7414, 3721, 9553, 6929, 5608, 6198, 3401, 7795, 4229, 7990, 7024, 6099, 8666, 5914, 1646, 8664, 4453, 9538, 2403, 8091, 2995, 5952, 9593, 9832, 7623, 8304, 9649, 432, 4399, 7773, 49, 1856, 9617, 4859, 3304, 7622, 2856, 7797, 5930, 4189, 9594, 8788, 894, 7843, 4875, 9819, 4855, 6614, 674, 3091, 9311, 972, 7109, 3538, 7028, 4352, 8306, 9646, 1873, 6282, 6273, 9931, 9550, 3407, 2139, 9879, 6498, 9847, 671, 3643, 8134, 1844, 9645, 9614, 2174, 164, 65, 3077, 4005, 9296, 9904, 6476, 2039, 645, 3088, 1604, 5062, 7601, 9073, 5932, 3041, 3399, 8148, 9587, 942, 4452, 9854, 102, 6436, 7677, 5578, 4580, 4142, 1057, 8543, 7439, 9623, 3099, 7792, 5196, 3705, 3449, 9934, 2123, 3087, 6501, 780, 9151, 6491, 4155, 3930, 5962, 768, 765, 5555, 7975, 2834, 8555, 2926, 8663, 9603, 2138, 4688, 9166, 9696, 772, 3542, 5957, 7485, 3334, 3553, 6086, 7315, 6916, 6773, 1167, 9641, 9654, 4659, 5311, 1303, 3537, 3972, 7440, 9552, 7638, 321, 323, 7390, 3541, 3871, 5357, 4, 9636, 5305, 9853, 3108, 5721, 7758, 3154, 3278, 3795, 4049, 7836, 1558, 6040, 9814, 1176, 1154, 9817, 8594, 6879, 6302, 9342, 3356, 2966, 1137, 4885, 8494, 1525, 1432, 4588, 6744, 253, 8499, 9451, 4246, 1254, 9426, 9850, 5641, 543, 7831, 3361, 9843, 7832, 9373, 6044, 5216, 9379, 3174, 8347, 5159, 8959, 2695, 2807, 6911, 9833, 712, 4827, 5936, 5482, 9158, 7850, 7772, 7536, 4274, 1121, 9851, 6931, 9826, 4358, 4295, 9483, 1584, 1275, 4395, 9029, 9816, 4271, 9806, 9616, 9580, 8419, 7546, 9852, 5329, 7867, 2697, 9841, 1470, 9821, 1191, 2555, 1768, 9597, 2938, 3762, 5724, 7140, 6707, 2119, 541, 4181, 9840, 9174, 2627, 7641, 9848, 7050, 9482, 1524, 9416, 3528, 9812, 3658, 3131, 7385, 8359, 4571, 4000, 8119, 9570, 6816, 9808, 1654, 9842, 9797, 9778, 9691, 9653, 9651, 9650, 9647, 9643, 9642, 9640, 9639, 9638, 9635, 9634, 9633, 9632, 9627, 9625, 9624, 9622, 9619, 9618, 9613, 9612, 9610, 9609, 9608, 9607, 9606, 9600, 9599, 9598, 9590, 9589, 9588, 9586, 9583, 9582, 9581, 9236, 9096, 8803, 8565, 8510, 8457, 8450, 8387, 8307, 8036, 8035, 8028, 8027, 7810, 7798, 7789, 7756, 7740, 7717, 7691, 7550, 7524, 7464, 7397, 7316, 7271, 7077, 7064, 6829, 6550, 6532, 6470, 6364, 6098, 6041, 5854, 5838, 5767, 5655, 5654, 5653, 5635, 5600, 5241, 5151, 5102, 5101, 5049, 4616, 4610, 4574, 4470, 4449, 4448, 4441, 4368, 4146, 3979, 3781, 3547, 3367, 3150, 3121, 3102, 3074, 2924, 2858, 2857, 2258, 2142, 2121, 2116, 2115, 2102, 1898, 1845, 1823, 1804, 1652, 1651, 1187, 1111, 1069, 913, 715, 714, 688, 655, 643, 630, 572, 571, 225, 209, 207, 112, 23]\n",
      "<class 'list'>\n",
      "CPU times: total: 31.2 ms\n",
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nix = rank(0, pro, comp, K, 0.5, 0.5, 1)\n",
    "print(nix)\n",
    "print(type(nix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lSM6p8AFt1Y"
   },
   "source": [
    "## 7.3 - Évaluation / Evaluation\n",
    "\n",
    "Vous allez tester différentes configurations du système de recommandations. Ces configurations seront comparées avec la [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Plus les discussions pertinentes sont recommandées rapidement (c.-à-d. en haut de la liste), plus élevé sera le score MAP. Ressources supplémentaires pour comprendre MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) et [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
    "\n",
    "\n",
    "La fonction *eval* évalue une configuration spécifique du système\n",
    "\n",
    "---\n",
    "\n",
    "We will test different configurations of our recommender system. These configurations are compared using the [mean average precision (MAP) metric](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision). Basically, the closer relevant threads are from ranked list begining, the higher MAP is. Additional materials to undertand MAP: [recall and precision over ranks](https://youtu.be/H7oAofuZjjE) and [MAP](https://youtu.be/pM6DJ0ZZee0).\n",
    "\n",
    "\n",
    "The function *eval* evaluates a specific configurantion of our system\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kyz5u-yCFt1Y"
   },
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "\n",
    "def calculate_map(x):\n",
    "    res = 0.0\n",
    "    n = 0.0\n",
    "    \n",
    "    for query_id, corrects, candidate_ids in x:\n",
    "        precisions = []\n",
    "        for k, candidate_id in enumerate(candidate_ids):\n",
    "            \n",
    "            if candidate_id in corrects:\n",
    "                prec_at_k = (len(precisions) + 1)/(k+1)\n",
    "                precisions.append(prec_at_k)\n",
    "                \n",
    "            if len(precisions) == len(corrects):\n",
    "                break\n",
    "                            \n",
    "        res += mean(precisions)\n",
    "        n += 1\n",
    "    \n",
    "    return res/n\n",
    "            \n",
    "\n",
    "def eval(tokenization_type, vectorizer, enable_stop_words, enable_stemming, w1, w2, w3):\n",
    "    reports = [r for r in report_index.values()]\n",
    "    report_ids = [r[\"report_id\"] for r in report_index.values()]\n",
    "    prod_v, comp_v, M = nlp_pipeline(reports, tokenization_type, vectorizer, enable_stop_words, enable_stemming)\n",
    "    report2idx = dict([(r['report_id'], idx) for idx,r in enumerate(reports)])\n",
    "    rank_lists = []\n",
    "    for query_id, corrects in test:\n",
    "        query_idx =  report_ids.index(query_id)\n",
    "        candidate_idxs = rank(query_idx, prod_v, comp_v, M, w1, w2, w3)\n",
    "        candidate_ids = [ report_ids[idx] for idx in candidate_idxs]\n",
    "                \n",
    "        rank_lists.append((query_id, corrects, candidate_ids))\n",
    "        \n",
    "        \n",
    "    return calculate_map(rank_lists)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMaHKg_WFt1a"
   },
   "source": [
    "## 7.4 - Question 9 (4 points)\n",
    "\n",
    "Évaluez votre système avec chacune des configurations suivantes :\n",
    "\n",
    "1. count(BoW) + space_tokenization\n",
    "2. count(BoW) + nltk_tokenization\n",
    "2. count(BoW) + space_punk_tokenization\n",
    "3. count(BoW) + space_punk_tokenization + Stop words removal\n",
    "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming\n",
    "5. tf_idf + space_punk_tokenization\n",
    "6. tf_idf + space_punk_tokenization + Stop words removal\n",
    "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming \n",
    "\n",
    "**Pour chaque configuration :** \n",
    "\n",
    "1. Rapportez la performance avec $w_1=0$ et $w_2=0$\n",
    "\n",
    "2. Réglez les paramètres $w_1$, $w_2$ et $w_3$ et rapportez les valeurs qui donne les 3 meilleurs et les 3 pires résultats.\n",
    "\n",
    "**En plus, décrivez et comparez vos résultats. Répondez aux questions suivantes :**\n",
    "\n",
    "- Quelle méthode de tokenization donne les meilleurs résultats ? À votre avis, pourquoi ?\n",
    "- Les étapes de prétraitement ont-elles un impact positif ou négatif sur notre système ?\n",
    "- Est-ce que TF-IDF permet d'obtenir de meilleures performances que CountBoW? Si oui, à votre avis, pourquoi ?\n",
    "- Est-ce que la comparaison du composant et du produit affecte positivement notre méthode ? \n",
    "\n",
    "**Notez qu'il y a une valeur minimum de  MAP à atteindre pour chaque configuration en dessous de laquelle la question sera pénalisée de 50%.**\n",
    "\n",
    "1. count(BoW) + space_tokenization: 0.090 \n",
    "2. count(BoW) + nltk_tokenization: 0.090\n",
    "2. count(BoW) + space_punk_tokenization: 0.120\n",
    "3. count(BoW) + space_punk_tokenization + Stop words removal: 0.170\n",
    "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming: 0.195\n",
    "5. tf_idf + space_punk_tokenization: 0.210\n",
    "6. tf_idf + space_punk_tokenization + Stop words removal: 0.210\n",
    "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming: 0.215\n",
    "\n",
    "---\n",
    "\n",
    "Evaluate the system using each one of the following configurations:\n",
    "\n",
    "1. count(BoW) + space_tokenization\n",
    "2. count(BoW) + nltk_tokenization\n",
    "2. count(BoW) + space_punk_tokenization\n",
    "3. count(BoW) + space_punk_tokenization + Stop words removal\n",
    "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming\n",
    "5. tf_idf + space_punk_tokenization\n",
    "6. tf_idf + space_punk_tokenization + Stop words removal\n",
    "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming \n",
    "\n",
    "**For each configuration:** \n",
    "\n",
    "1. Report the method performance achieved when $w_1=0$ and $w_2=0$\n",
    "\n",
    "2. Tune the parameters $w_1$, $w_2$ and $w_3$ and report the parameter values that achieve the 3 best and 3 worst results.\n",
    "\n",
    "**Also, describe and compare the results found by you and answer the following questions:**\n",
    "\n",
    "- Which tokenization strategy has achieved the best result? Why do you think this has occurred?\n",
    "- Was our system negatively or positively impacted by data preprocessing steps?\n",
    "- TF-IDF has achieved a better performance than CountBoW? If yes, why do you think that this has occurred?\n",
    "- Did the component and product comparison positively affect our method? \n",
    "\n",
    "**Note that there is a minimum MAP value to achieve for each configuration (see below) below which the question will be penalized by 50%.**\n",
    "\n",
    "1. count(BoW) + space_tokenization: 0.090 \n",
    "2. count(BoW) + nltk_tokenization: 0.090\n",
    "2. count(BoW) + space_punk_tokenization: 0.120\n",
    "3. count(BoW) + space_punk_tokenization + Stop words removal: 0.170\n",
    "4. count(BoW) + space_punk_tokenization + Stop words removal + Stemming: 0.195\n",
    "5. tf_idf + space_punk_tokenization: 0.210\n",
    "6. tf_idf + space_punk_tokenization + Stop words removal: 0.210\n",
    "7. tf_idf + space_punk_tokenization + Stop words removal + Stemming: 0.215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [(0,0,1), (1,0,0), (0,1,0), (0.33, 0.33, 0.33), (0.25, 0.25, 0.5), (0.1, 0.1, 0.8), (0.05, 0.05, 0.9), (0.025, 0.025, 0.95), (0.0125, 0.0125, 0.975), (0.01, 0.01, 0.98)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def W_eval_dataframe(tokenization_type, vectorizer, enable_stop_words, enable_stemming, W):\n",
    "    \n",
    "    eval_dict = {\n",
    "        'w1':[w1 for w1,w2,w3 in W],\n",
    "        'w2':[w2 for w1,w2,w3 in W],\n",
    "        'w3':[w3 for w1,w2,w3 in W],\n",
    "        'MAP':[eval(tokenization_type, vectorizer, enable_stop_words, enable_stemming, w1, w2, w3) for (w1,w2,w3) in tqdm(W)]\n",
    "    }\n",
    "    \n",
    "    W_eval_df = pd.DataFrame(eval_dict, index= [f'Combination {i}' for i in range(1,len(W)+1)])\n",
    "    \n",
    "    print(f\"---> The performance of {len(W)} different combinations of (w1,w2,w3):\")\n",
    "    print(tabulate(W_eval_df, headers = 'keys', tablefmt = 'psql'))\n",
    "    print('\\n')\n",
    "    \n",
    "    three_best = W_eval_df.sort_values(by = ['MAP'], ascending = False)[:3]\n",
    "    three_best.index = [1,2,3]\n",
    "    three_worst = W_eval_df.sort_values(by = ['MAP'])[:3]\n",
    "    three_worst.index = [1,2,3]\n",
    "    \n",
    "    print('---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:')\n",
    "    print(tabulate(three_best, headers = 'keys', tablefmt = 'psql'))\n",
    "    print('\\n')\n",
    "    print('---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:')\n",
    "    print(tabulate(three_worst, headers = 'keys', tablefmt = 'psql'))\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. count(BoW) + space_tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [02:03<00:00, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> The performance of 10 different combinations of (w1,w2,w3):\n",
      "+----------------+--------+--------+-------+------------+\n",
      "|                |     w1 |     w2 |    w3 |        MAP |\n",
      "|----------------+--------+--------+-------+------------|\n",
      "| Combination 1  | 0      | 0      | 1     | 0.0838898  |\n",
      "| Combination 2  | 1      | 0      | 0     | 0.00391105 |\n",
      "| Combination 3  | 0      | 1      | 0     | 0.0136751  |\n",
      "| Combination 4  | 0.33   | 0.33   | 0.33  | 0.0881391  |\n",
      "| Combination 5  | 0.25   | 0.25   | 0.5   | 0.0877804  |\n",
      "| Combination 6  | 0.1    | 0.1    | 0.8   | 0.095196   |\n",
      "| Combination 7  | 0.05   | 0.05   | 0.9   | 0.0903597  |\n",
      "| Combination 8  | 0.025  | 0.025  | 0.95  | 0.087135   |\n",
      "| Combination 9  | 0.0125 | 0.0125 | 0.975 | 0.0874612  |\n",
      "| Combination 10 | 0.01   | 0.01   | 0.98  | 0.0873582  |\n",
      "+----------------+--------+--------+-------+------------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:\n",
      "+----+------+------+------+-----------+\n",
      "|    |   w1 |   w2 |   w3 |       MAP |\n",
      "|----+------+------+------+-----------|\n",
      "|  1 | 0.1  | 0.1  | 0.8  | 0.095196  |\n",
      "|  2 | 0.05 | 0.05 | 0.9  | 0.0903597 |\n",
      "|  3 | 0.33 | 0.33 | 0.33 | 0.0881391 |\n",
      "+----+------+------+------+-----------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:\n",
      "+----+------+------+------+------------+\n",
      "|    |   w1 |   w2 |   w3 |        MAP |\n",
      "|----+------+------+------+------------|\n",
      "|  1 |    1 |    0 |    0 | 0.00391105 |\n",
      "|  2 |    0 |    1 |    0 | 0.0136751  |\n",
      "|  3 |    0 |    0 |    1 | 0.0838898  |\n",
      "+----+------+------+------+------------+\n",
      "\n",
      "\n",
      "CPU times: total: 2min 2s\n",
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W_eval_dataframe(tokenize_space, transform_count_bow, False, False, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. count(BoW) + nltk_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [04:01<00:00, 24.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> The performance of 10 different combinations of (w1,w2,w3):\n",
      "+----------------+--------+--------+-------+------------+\n",
      "|                |     w1 |     w2 |    w3 |        MAP |\n",
      "|----------------+--------+--------+-------+------------|\n",
      "| Combination 1  | 0      | 0      | 1     | 0.0820239  |\n",
      "| Combination 2  | 1      | 0      | 0     | 0.00391105 |\n",
      "| Combination 3  | 0      | 1      | 0     | 0.0136751  |\n",
      "| Combination 4  | 0.33   | 0.33   | 0.33  | 0.0894025  |\n",
      "| Combination 5  | 0.25   | 0.25   | 0.5   | 0.089545   |\n",
      "| Combination 6  | 0.1    | 0.1    | 0.8   | 0.0956418  |\n",
      "| Combination 7  | 0.05   | 0.05   | 0.9   | 0.0970113  |\n",
      "| Combination 8  | 0.025  | 0.025  | 0.95  | 0.0886784  |\n",
      "| Combination 9  | 0.0125 | 0.0125 | 0.975 | 0.0870183  |\n",
      "| Combination 10 | 0.01   | 0.01   | 0.98  | 0.0859703  |\n",
      "+----------------+--------+--------+-------+------------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:\n",
      "+----+------+------+------+-----------+\n",
      "|    |   w1 |   w2 |   w3 |       MAP |\n",
      "|----+------+------+------+-----------|\n",
      "|  1 | 0.05 | 0.05 |  0.9 | 0.0970113 |\n",
      "|  2 | 0.1  | 0.1  |  0.8 | 0.0956418 |\n",
      "|  3 | 0.25 | 0.25 |  0.5 | 0.089545  |\n",
      "+----+------+------+------+-----------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:\n",
      "+----+------+------+------+------------+\n",
      "|    |   w1 |   w2 |   w3 |        MAP |\n",
      "|----+------+------+------+------------|\n",
      "|  1 |    1 |    0 |    0 | 0.00391105 |\n",
      "|  2 |    0 |    1 |    0 | 0.0136751  |\n",
      "|  3 |    0 |    0 |    1 | 0.0820239  |\n",
      "+----+------+------+------+------------+\n",
      "\n",
      "\n",
      "CPU times: total: 3min 59s\n",
      "Wall time: 4min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W_eval_dataframe(tokenize_nltk, transform_count_bow, False, False, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. count(BoW) + space_punk_tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [02:17<00:00, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> The performance of 10 different combinations of (w1,w2,w3):\n",
      "+----------------+--------+--------+-------+------------+\n",
      "|                |     w1 |     w2 |    w3 |        MAP |\n",
      "|----------------+--------+--------+-------+------------|\n",
      "| Combination 1  | 0      | 0      | 1     | 0.119054   |\n",
      "| Combination 2  | 1      | 0      | 0     | 0.00391105 |\n",
      "| Combination 3  | 0      | 1      | 0     | 0.0136751  |\n",
      "| Combination 4  | 0.33   | 0.33   | 0.33  | 0.119447   |\n",
      "| Combination 5  | 0.25   | 0.25   | 0.5   | 0.119205   |\n",
      "| Combination 6  | 0.1    | 0.1    | 0.8   | 0.129894   |\n",
      "| Combination 7  | 0.05   | 0.05   | 0.9   | 0.12997    |\n",
      "| Combination 8  | 0.025  | 0.025  | 0.95  | 0.126752   |\n",
      "| Combination 9  | 0.0125 | 0.0125 | 0.975 | 0.121513   |\n",
      "| Combination 10 | 0.01   | 0.01   | 0.98  | 0.121328   |\n",
      "+----------------+--------+--------+-------+------------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:\n",
      "+----+-------+-------+------+----------+\n",
      "|    |    w1 |    w2 |   w3 |      MAP |\n",
      "|----+-------+-------+------+----------|\n",
      "|  1 | 0.05  | 0.05  | 0.9  | 0.12997  |\n",
      "|  2 | 0.1   | 0.1   | 0.8  | 0.129894 |\n",
      "|  3 | 0.025 | 0.025 | 0.95 | 0.126752 |\n",
      "+----+-------+-------+------+----------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:\n",
      "+----+------+------+------+------------+\n",
      "|    |   w1 |   w2 |   w3 |        MAP |\n",
      "|----+------+------+------+------------|\n",
      "|  1 |    1 |    0 |    0 | 0.00391105 |\n",
      "|  2 |    0 |    1 |    0 | 0.0136751  |\n",
      "|  3 |    0 |    0 |    1 | 0.119054   |\n",
      "+----+------+------+------+------------+\n",
      "\n",
      "\n",
      "CPU times: total: 2min 15s\n",
      "Wall time: 2min 17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W_eval_dataframe(tokenize_space_punk, transform_count_bow, False, False, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. count(BoW) + space_punk_tokenization + Stop words removal \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:58<00:00, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> The performance of 10 different combinations of (w1,w2,w3):\n",
      "+----------------+--------+--------+-------+------------+\n",
      "|                |     w1 |     w2 |    w3 |        MAP |\n",
      "|----------------+--------+--------+-------+------------|\n",
      "| Combination 1  | 0      | 0      | 1     | 0.167225   |\n",
      "| Combination 2  | 1      | 0      | 0     | 0.00391105 |\n",
      "| Combination 3  | 0      | 1      | 0     | 0.0136751  |\n",
      "| Combination 4  | 0.33   | 0.33   | 0.33  | 0.143961   |\n",
      "| Combination 5  | 0.25   | 0.25   | 0.5   | 0.143827   |\n",
      "| Combination 6  | 0.1    | 0.1    | 0.8   | 0.171352   |\n",
      "| Combination 7  | 0.05   | 0.05   | 0.9   | 0.178748   |\n",
      "| Combination 8  | 0.025  | 0.025  | 0.95  | 0.173612   |\n",
      "| Combination 9  | 0.0125 | 0.0125 | 0.975 | 0.169366   |\n",
      "| Combination 10 | 0.01   | 0.01   | 0.98  | 0.16848    |\n",
      "+----------------+--------+--------+-------+------------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:\n",
      "+----+-------+-------+------+----------+\n",
      "|    |    w1 |    w2 |   w3 |      MAP |\n",
      "|----+-------+-------+------+----------|\n",
      "|  1 | 0.05  | 0.05  | 0.9  | 0.178748 |\n",
      "|  2 | 0.025 | 0.025 | 0.95 | 0.173612 |\n",
      "|  3 | 0.1   | 0.1   | 0.8  | 0.171352 |\n",
      "+----+-------+-------+------+----------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:\n",
      "+----+------+------+------+------------+\n",
      "|    |   w1 |   w2 |   w3 |        MAP |\n",
      "|----+------+------+------+------------|\n",
      "|  1 | 1    | 0    |  0   | 0.00391105 |\n",
      "|  2 | 0    | 1    |  0   | 0.0136751  |\n",
      "|  3 | 0.25 | 0.25 |  0.5 | 0.143827   |\n",
      "+----+------+------+------+------------+\n",
      "\n",
      "\n",
      "CPU times: total: 1min 56s\n",
      "Wall time: 1min 58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W_eval_dataframe(tokenize_space_punk, transform_count_bow, True, False, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. count(BoW) + space_punk_tokenization + Stop words removal + Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [04:03<00:00, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> The performance of 10 different combinations of (w1,w2,w3):\n",
      "+----------------+--------+--------+-------+------------+\n",
      "|                |     w1 |     w2 |    w3 |        MAP |\n",
      "|----------------+--------+--------+-------+------------|\n",
      "| Combination 1  | 0      | 0      | 1     | 0.194213   |\n",
      "| Combination 2  | 1      | 0      | 0     | 0.00391105 |\n",
      "| Combination 3  | 0      | 1      | 0     | 0.0136751  |\n",
      "| Combination 4  | 0.33   | 0.33   | 0.33  | 0.155522   |\n",
      "| Combination 5  | 0.25   | 0.25   | 0.5   | 0.155455   |\n",
      "| Combination 6  | 0.1    | 0.1    | 0.8   | 0.194238   |\n",
      "| Combination 7  | 0.05   | 0.05   | 0.9   | 0.200108   |\n",
      "| Combination 8  | 0.025  | 0.025  | 0.95  | 0.196189   |\n",
      "| Combination 9  | 0.0125 | 0.0125 | 0.975 | 0.199024   |\n",
      "| Combination 10 | 0.01   | 0.01   | 0.98  | 0.199868   |\n",
      "+----------------+--------+--------+-------+------------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:\n",
      "+----+--------+--------+-------+----------+\n",
      "|    |     w1 |     w2 |    w3 |      MAP |\n",
      "|----+--------+--------+-------+----------|\n",
      "|  1 | 0.05   | 0.05   | 0.9   | 0.200108 |\n",
      "|  2 | 0.01   | 0.01   | 0.98  | 0.199868 |\n",
      "|  3 | 0.0125 | 0.0125 | 0.975 | 0.199024 |\n",
      "+----+--------+--------+-------+----------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:\n",
      "+----+------+------+------+------------+\n",
      "|    |   w1 |   w2 |   w3 |        MAP |\n",
      "|----+------+------+------+------------|\n",
      "|  1 | 1    | 0    |  0   | 0.00391105 |\n",
      "|  2 | 0    | 1    |  0   | 0.0136751  |\n",
      "|  3 | 0.25 | 0.25 |  0.5 | 0.155455   |\n",
      "+----+------+------+------+------------+\n",
      "\n",
      "\n",
      "CPU times: total: 4min 1s\n",
      "Wall time: 4min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W_eval_dataframe(tokenize_space_punk, transform_count_bow, True, True, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. tf_idf + space_punk_tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [02:11<00:00, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> The performance of 10 different combinations of (w1,w2,w3):\n",
      "+----------------+--------+--------+-------+------------+\n",
      "|                |     w1 |     w2 |    w3 |        MAP |\n",
      "|----------------+--------+--------+-------+------------|\n",
      "| Combination 1  | 0      | 0      | 1     | 0.201861   |\n",
      "| Combination 2  | 1      | 0      | 0     | 0.00391105 |\n",
      "| Combination 3  | 0      | 1      | 0     | 0.0136751  |\n",
      "| Combination 4  | 0.33   | 0.33   | 0.33  | 0.158464   |\n",
      "| Combination 5  | 0.25   | 0.25   | 0.5   | 0.158331   |\n",
      "| Combination 6  | 0.1    | 0.1    | 0.8   | 0.204455   |\n",
      "| Combination 7  | 0.05   | 0.05   | 0.9   | 0.214397   |\n",
      "| Combination 8  | 0.025  | 0.025  | 0.95  | 0.210077   |\n",
      "| Combination 9  | 0.0125 | 0.0125 | 0.975 | 0.206889   |\n",
      "| Combination 10 | 0.01   | 0.01   | 0.98  | 0.205866   |\n",
      "+----------------+--------+--------+-------+------------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:\n",
      "+----+--------+--------+-------+----------+\n",
      "|    |     w1 |     w2 |    w3 |      MAP |\n",
      "|----+--------+--------+-------+----------|\n",
      "|  1 | 0.05   | 0.05   | 0.9   | 0.214397 |\n",
      "|  2 | 0.025  | 0.025  | 0.95  | 0.210077 |\n",
      "|  3 | 0.0125 | 0.0125 | 0.975 | 0.206889 |\n",
      "+----+--------+--------+-------+----------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:\n",
      "+----+------+------+------+------------+\n",
      "|    |   w1 |   w2 |   w3 |        MAP |\n",
      "|----+------+------+------+------------|\n",
      "|  1 | 1    | 0    |  0   | 0.00391105 |\n",
      "|  2 | 0    | 1    |  0   | 0.0136751  |\n",
      "|  3 | 0.25 | 0.25 |  0.5 | 0.158331   |\n",
      "+----+------+------+------+------------+\n",
      "\n",
      "\n",
      "CPU times: total: 2min 10s\n",
      "Wall time: 2min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W_eval_dataframe(tokenize_space_punk, transform_tf_idf_bow, False, False, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  tf_idf + space_punk_tokenization + Stop words removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:59<00:00, 11.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> The performance of 10 different combinations of (w1,w2,w3):\n",
      "+----------------+--------+--------+-------+------------+\n",
      "|                |     w1 |     w2 |    w3 |        MAP |\n",
      "|----------------+--------+--------+-------+------------|\n",
      "| Combination 1  | 0      | 0      | 1     | 0.19926    |\n",
      "| Combination 2  | 1      | 0      | 0     | 0.00391105 |\n",
      "| Combination 3  | 0      | 1      | 0     | 0.0136751  |\n",
      "| Combination 4  | 0.33   | 0.33   | 0.33  | 0.158668   |\n",
      "| Combination 5  | 0.25   | 0.25   | 0.5   | 0.158557   |\n",
      "| Combination 6  | 0.1    | 0.1    | 0.8   | 0.203172   |\n",
      "| Combination 7  | 0.05   | 0.05   | 0.9   | 0.212298   |\n",
      "| Combination 8  | 0.025  | 0.025  | 0.95  | 0.205445   |\n",
      "| Combination 9  | 0.0125 | 0.0125 | 0.975 | 0.204717   |\n",
      "| Combination 10 | 0.01   | 0.01   | 0.98  | 0.203802   |\n",
      "+----------------+--------+--------+-------+------------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:\n",
      "+----+--------+--------+-------+----------+\n",
      "|    |     w1 |     w2 |    w3 |      MAP |\n",
      "|----+--------+--------+-------+----------|\n",
      "|  1 | 0.05   | 0.05   | 0.9   | 0.212298 |\n",
      "|  2 | 0.025  | 0.025  | 0.95  | 0.205445 |\n",
      "|  3 | 0.0125 | 0.0125 | 0.975 | 0.204717 |\n",
      "+----+--------+--------+-------+----------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:\n",
      "+----+------+------+------+------------+\n",
      "|    |   w1 |   w2 |   w3 |        MAP |\n",
      "|----+------+------+------+------------|\n",
      "|  1 | 1    | 0    |  0   | 0.00391105 |\n",
      "|  2 | 0    | 1    |  0   | 0.0136751  |\n",
      "|  3 | 0.25 | 0.25 |  0.5 | 0.158557   |\n",
      "+----+------+------+------+------------+\n",
      "\n",
      "\n",
      "CPU times: total: 1min 57s\n",
      "Wall time: 1min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W_eval_dataframe(tokenize_space_punk, transform_tf_idf_bow, True, False, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.tf_idf + space_punk_tokenization + Stop words removal + Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [04:16<00:00, 25.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> The performance of 10 different combinations of (w1,w2,w3):\n",
      "+----------------+--------+--------+-------+------------+\n",
      "|                |     w1 |     w2 |    w3 |        MAP |\n",
      "|----------------+--------+--------+-------+------------|\n",
      "| Combination 1  | 0      | 0      | 1     | 0.212493   |\n",
      "| Combination 2  | 1      | 0      | 0     | 0.00391105 |\n",
      "| Combination 3  | 0      | 1      | 0     | 0.0136751  |\n",
      "| Combination 4  | 0.33   | 0.33   | 0.33  | 0.165059   |\n",
      "| Combination 5  | 0.25   | 0.25   | 0.5   | 0.16522    |\n",
      "| Combination 6  | 0.1    | 0.1    | 0.8   | 0.217289   |\n",
      "| Combination 7  | 0.05   | 0.05   | 0.9   | 0.224494   |\n",
      "| Combination 8  | 0.025  | 0.025  | 0.95  | 0.222491   |\n",
      "| Combination 9  | 0.0125 | 0.0125 | 0.975 | 0.216999   |\n",
      "| Combination 10 | 0.01   | 0.01   | 0.98  | 0.215698   |\n",
      "+----------------+--------+--------+-------+------------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 best MAP values are:\n",
      "+----+-------+-------+------+----------+\n",
      "|    |    w1 |    w2 |   w3 |      MAP |\n",
      "|----+-------+-------+------+----------|\n",
      "|  1 | 0.05  | 0.05  | 0.9  | 0.224494 |\n",
      "|  2 | 0.025 | 0.025 | 0.95 | 0.222491 |\n",
      "|  3 | 0.1   | 0.1   | 0.8  | 0.217289 |\n",
      "+----+-------+-------+------+----------+\n",
      "\n",
      "\n",
      "---> The parameters (w1,w2,w3) that achieve the 3 worst MAP values are:\n",
      "+----+------+------+------+------------+\n",
      "|    |   w1 |   w2 |   w3 |        MAP |\n",
      "|----+------+------+------+------------|\n",
      "|  1 | 1    | 0    | 0    | 0.00391105 |\n",
      "|  2 | 0    | 1    | 0    | 0.0136751  |\n",
      "|  3 | 0.33 | 0.33 | 0.33 | 0.165059   |\n",
      "+----+------+------+------+------------+\n",
      "\n",
      "\n",
      "CPU times: total: 4min 13s\n",
      "Wall time: 4min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "W_eval_dataframe(tokenize_space_punk, transform_tf_idf_bow, True, True, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the results:\n",
    "\n",
    "#### 1.  The tokenization strategy:\n",
    "The tokenization strategy has achieved the best result is space_punk_tokenization since this strategy eliminates punctuations, knowing that punctuations tends to occur in the bug reports description, including punctuation when calculating the similarity is a bad idea, since it doesn't give any insight about similarity, so it only alters the ranking of similar reports to the query, that's why space_punk_tokenization achieved the best results in all of the configurations, this knowing that the two other tokenizations strategies keep the punctuations as tokens.\n",
    "\n",
    "#### 2.  The impact of data preprocessing steps on our system:\n",
    "We notice that our system has been impacted positively with our data preprocessing steps and this for the two vectorizers: count_bow and tf_idf and this for most of the combinations used on the configurations, each time we rach higher MAP values (by removing stop words, and we reach higher MAP values with removing stop words and enable stemming). \n",
    "\n",
    "#### 3.  TF-IDF vs Count-BOW:\n",
    "TF-IDF showed better results than count_bow vectorization for the same parameters combinations and the same data preprocessing steps, because TF-IDF penalizes the frequent words that are in most of documents, because these words don't discriminate documents or gives an insight on the similarity of documents, but in the other hand this strategy rewards the rarer words, that's why we reach better results. \n",
    "\n",
    "#### 4.  The impact of the component and product comparison on our system:\n",
    "The product and component comparison defenitely affect positively our system, We notice that we get the higher MAP values for any type of vectorization, and data processing steps non values for w1 and w2 compared to when w1=0 and w2=0. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Laboratoire_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
